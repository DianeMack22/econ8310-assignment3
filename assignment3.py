{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyManaJt3mM1WVSOW05q0l5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DianeMack22/econ8310-assignment3/blob/main/assignment3.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deym69gGA-oK"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 1. Install and import tools\n",
        "# -----------------------------\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import struct\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Download .gz files from Google Drive\n",
        "# -----------------------------\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "drive_files = {\n",
        "    # Train images\n",
        "    \"1pe4h0HUyugjAvSp8-xUbK61svrkcnF4x\": \"data/train-images-idx3-ubyte.gz\",\n",
        "    # Train labels\n",
        "    \"12vOBpJKWuW2_y5R__Dv16IiGJ1YPJiq2\": \"data/train-labels-idx1-ubyte.gz\",\n",
        "    # Test images\n",
        "    \"1F7k0T5nC0XDufouFzU9QB0LPcfxcEFkx\": \"data/t10k-images-idx3-ubyte.gz\",\n",
        "    # Test labels\n",
        "    \"1wWYt5HKjf1s-R9XzfkaYQesuOL7BZFHM\": \"data/t10k-labels-idx1-ubyte.gz\"\n",
        "}\n",
        "\n",
        "for file_id, dest_path in drive_files.items():\n",
        "    if not os.path.exists(dest_path):\n",
        "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", dest_path, quiet=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Extract all .gz files\n",
        "# -----------------------------\n",
        "gz_files = list(drive_files.values())  # use the same files downloaded\n",
        "\n",
        "for gz_path in gz_files:\n",
        "    out_path = gz_path[:-3]  # remove '.gz'\n",
        "    if not os.path.exists(out_path):\n",
        "        with gzip.open(gz_path, 'rb') as f_in:\n",
        "            with open(out_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "# Note: ChatGPT, Google, and Gemini assisted significantly with Sections 2-3, and\n",
        "# had some influence in the debugging process for the rest of the code.\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Define CustomMNIST Dataset\n",
        "# -----------------------------\n",
        "class CustomMNIST(Dataset):\n",
        "    def __init__(self, image_path, label_path, transform=None):\n",
        "        self.images = self._read_images(image_path)\n",
        "        self.labels = self._read_labels(label_path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def _read_images(self, path):\n",
        "      with open(path, 'rb') as f:\n",
        "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
        "        expected_size = num * rows * cols\n",
        "        image_data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "\n",
        "        print(f\"Image data shape: {image_data.shape}, Expected shape: ({num}, {rows}, {cols})\")\n",
        "\n",
        "        if image_data.size < expected_size:\n",
        "            num_actual = image_data.size // (rows * cols)\n",
        "            print(f\"Reshape failed, adjusted number of images to: {num_actual}\")\n",
        "            image_data = image_data[:num_actual * rows * cols]\n",
        "            images = image_data.reshape((num_actual, rows, cols))\n",
        "        else:\n",
        "            images = image_data.reshape((num, rows, cols))\n",
        "      return images\n",
        "\n",
        "    def _read_labels(self, path):\n",
        "      with open(path, 'rb') as f:\n",
        "        magic, num = struct.unpack(\">II\", f.read(8))\n",
        "        label_data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "\n",
        "        if label_data.size != num:\n",
        "            print(f\"Label count mismatch: expected {num}, got {label_data.size}\")\n",
        "            num_actual = min(num, label_data.size)\n",
        "            label_data = label_data[:num_actual]\n",
        "        return label_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Normalize and add channel dimension [1, 28, 28]\n",
        "            image = torch.tensor(image, dtype=torch.float32).unsqueeze(0) / 255.0\n",
        "        return image, label\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Create datasets and DataLoaders\n",
        "# -----------------------------\n",
        "train_dataset = CustomMNIST(\n",
        "    image_path=\"data/train-images-idx3-ubyte\",\n",
        "    label_path=\"data/train-labels-idx1-ubyte\"\n",
        ")\n",
        "\n",
        "test_dataset = CustomMNIST(\n",
        "    image_path=\"data/t10k-images-idx3-ubyte\",\n",
        "    label_path=\"data/t10k-labels-idx1-ubyte\"\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Create model\n",
        "# -----------------------------\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FirstNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    # Define the components of the model\n",
        "    super(FirstNet, self).__init__()\n",
        "    # Function to flatten our image\n",
        "    self.flatten = nn.Flatten()\n",
        "    # Create the sequence of our network\n",
        "    self.linear_relu_model = nn.Sequential(\n",
        "        # Add a linear output layer with 10 perceptrons\n",
        "        nn.LazyLinear(10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Construct the sequencing of the model here\n",
        "    x = self.flatten(x)\n",
        "    # Pass flattened images through our sequence\n",
        "    output = self.linear_relu_model(x)\n",
        "\n",
        "    # Return the evaluations of our ten classes as a 10-dimensional vector\n",
        "    return output\n",
        "\n",
        "  # Create an instance of our model\n",
        "model = FirstNet()\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Prepare to Train\n",
        "# -----------------------------\n",
        "# Define training parameters\n",
        "learning_rate = 1e-1\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "# Define the loss function, for multiclass problems\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Build our optimizer with the parameters from the model we defined, and the learning rate we selected\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  # Set the model to training mode, important for batch normalization & dropout layers\n",
        "  model.train()\n",
        "  # Loop over batches via the dataloader\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation and looking for improved gradients\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Zeroing out the gradient (otherwise they are summed) in prep for next round\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Print progress update every few loops\n",
        "    if batch % 10 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "  # -----------------------------\n",
        "# 8. Prepare to Train AND Test the Model\n",
        "# -----------------------------\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  # Evaluating the model with torch.no_grad()\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) ==y).type(torch.float).sum().item()\n",
        "\n",
        "  # Printing some output after a testing round\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "  # -----------------------------\n",
        "# 9. Train the Model\n",
        "# -----------------------------\n",
        "# Repeat the training process for each epoch\n",
        "\n",
        "# -----------------------------\n",
        "# 9â€“11. Train the Model, Save, and Load\n",
        "# -----------------------------\n",
        "def run_training():\n",
        "    model = FirstNet()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1} \\n---------------------------\")\n",
        "        train_loop(train_loader, model, loss_fn, optimizer)\n",
        "        test_loop(test_loader, model, loss_fn)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "    # Save the model\n",
        "    PATH = \"model1.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epochs,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss_fn,\n",
        "    }, PATH)\n",
        "\n",
        "\n",
        "def load_model(path=\"model1.pt\"):\n",
        "    model = FirstNet()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    checkpoint = torch.load(path, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    model.eval()\n",
        "    return model, optimizer, epoch, loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_training()"
      ]
    }
  ]
}