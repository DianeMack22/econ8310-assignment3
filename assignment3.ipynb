{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpxeyyoCvKtWsEw4xWIlo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DianeMack22/econ8310-assignment3/blob/main/assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzX1vYct-OWa",
        "outputId": "cac49dea-c848-45d1-b0e2-4154178bdd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image data shape: (47040000,), Expected shape: (60000, 28, 28)\n",
            "Image data shape: (7840000,), Expected shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# 1. Install and import tools\n",
        "# -----------------------------\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import struct\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Download .gz files from Google Drive\n",
        "# -----------------------------\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "drive_files = {\n",
        "    # Train images\n",
        "    \"1pe4h0HUyugjAvSp8-xUbK61svrkcnF4x\": \"data/train-images-idx3-ubyte.gz\",\n",
        "    # Train labels\n",
        "    \"12vOBpJKWuW2_y5R__Dv16IiGJ1YPJiq2\": \"data/train-labels-idx1-ubyte.gz\",\n",
        "    # Test images\n",
        "    \"1F7k0T5nC0XDufouFzU9QB0LPcfxcEFkx\": \"data/t10k-images-idx3-ubyte.gz\",\n",
        "    # Test labels\n",
        "    \"1wWYt5HKjf1s-R9XzfkaYQesuOL7BZFHM\": \"data/t10k-labels-idx1-ubyte.gz\"\n",
        "}\n",
        "\n",
        "for file_id, dest_path in drive_files.items():\n",
        "    if not os.path.exists(dest_path):\n",
        "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", dest_path, quiet=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Extract all .gz files\n",
        "# -----------------------------\n",
        "gz_files = list(drive_files.values())  # use the same files we downloaded\n",
        "\n",
        "for gz_path in gz_files:\n",
        "    out_path = gz_path[:-3]  # remove '.gz'\n",
        "    if not os.path.exists(out_path):\n",
        "        with gzip.open(gz_path, 'rb') as f_in:\n",
        "            with open(out_path, 'wb') as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "# Note: ChatGPT, Google, and Gemini assisted significantly with Sections 2-3, and\n",
        "# had some influence in the debugging process for the rest of the code.\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Define CustomMNIST Dataset\n",
        "# -----------------------------\n",
        "class CustomMNIST(Dataset):\n",
        "    def __init__(self, image_path, label_path, transform=None):\n",
        "        self.images = self._read_images(image_path)\n",
        "        self.labels = self._read_labels(label_path)\n",
        "        self.transform = transform\n",
        "\n",
        "    def _read_images(self, path):\n",
        "      with open(path, 'rb') as f:\n",
        "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
        "        expected_size = num * rows * cols\n",
        "        image_data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "\n",
        "        print(f\"Image data shape: {image_data.shape}, Expected shape: ({num}, {rows}, {cols})\")\n",
        "\n",
        "        if image_data.size < expected_size:\n",
        "            num_actual = image_data.size // (rows * cols)\n",
        "            print(f\"Reshape failed, adjusted number of images to: {num_actual}\")\n",
        "            image_data = image_data[:num_actual * rows * cols]\n",
        "            images = image_data.reshape((num_actual, rows, cols))\n",
        "        else:\n",
        "            images = image_data.reshape((num, rows, cols))\n",
        "      return images\n",
        "\n",
        "    def _read_labels(self, path):\n",
        "      with open(path, 'rb') as f:\n",
        "        magic, num = struct.unpack(\">II\", f.read(8))\n",
        "        label_data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "\n",
        "        if label_data.size != num:\n",
        "            print(f\"Label count mismatch: expected {num}, got {label_data.size}\")\n",
        "            num_actual = min(num, label_data.size)\n",
        "            label_data = label_data[:num_actual]\n",
        "        return label_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Normalize and add channel dimension [1, 28, 28]\n",
        "            image = torch.tensor(image, dtype=torch.float32).unsqueeze(0) / 255.0\n",
        "        return image, label\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Create datasets and DataLoaders\n",
        "# -----------------------------\n",
        "train_dataset = CustomMNIST(\n",
        "    image_path=\"data/train-images-idx3-ubyte\",\n",
        "    label_path=\"data/train-labels-idx1-ubyte\"\n",
        ")\n",
        "\n",
        "test_dataset = CustomMNIST(\n",
        "    image_path=\"data/t10k-images-idx3-ubyte\",\n",
        "    label_path=\"data/t10k-labels-idx1-ubyte\"\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "idx = 100\n",
        "\n",
        "image, label = train_dataset[idx]\n",
        "print(f\"This image is labeled a {label}\")\n",
        "px.imshow(image.squeeze().numpy(), color_continuous_scale=\"gray\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "_y3Kkw5RAKFf",
        "outputId": "2ddece21-063e-4ad0-c2fc-b682384ee937"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image is labeled a 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"3f0e1dd6-c869-4a12-8d22-f2b9398178ca\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3f0e1dd6-c869-4a12-8d22-f2b9398178ca\")) {                    Plotly.newPlot(                        \"3f0e1dd6-c869-4a12-8d22-f2b9398178ca\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.003921569,0.0,0.0,0.07058824,0.41960785,0.46666667,0.40392157,0.03529412,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3882353,0.60784316,0.44313726,0.23921569,0.4627451,0.6784314,0.45882353,0.0,0.0,0.011764706,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.003921569,0.0,0.43137255,0.53333336,0.0,0.0,0.0,0.0,0.0,0.654902,0.62352943,0.0,0.0,0.007843138,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.003921569,0.0,0.023529412,0.0,0.28235295,0.5764706,0.0,0.0,0.019607844,0.0,0.007843138,0.0,0.0,0.68235296,0.4627451,0.0,0.019607844,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.003921569,0.019607844,0.0,0.0,0.68235296,0.015686275,0.0,0.015686275,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.17254902,0.0,0.015686275,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.03137255,0.0,0.49019608,0.5019608,0.0,0.03529412,0.0,0.0,0.0,0.0,0.0,0.007843138,0.0,0.41960785,0.59607846,0.0,0.023529412,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.72156864,0.03529412,0.0,0.015686275,0.0,0.0,0.0,0.0,0.0,0.007843138,0.0,0.0,0.74509805,0.0,0.0,0.007843138,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.003921569,0.0,0.007843138,0.0,0.28627452,0.61960787,0.0,0.0,0.0,0.0,0.003921569,0.0,0.0,0.0,0.0,0.015686275,0.0,0.6156863,0.30980393,0.0,0.011764706,0.003921569,0.0,0.0,0.0],[0.0,0.0,0.007843138,0.019607844,0.003921569,0.0,0.0,0.6156863,0.38431373,0.0,0.007843138,0.0,0.0,0.003921569,0.0,0.0,0.0,0.0,0.019607844,0.0,0.4745098,0.5686275,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.90588236,0.32941177,0.0,0.007843138,0.003921569,0.003921569,0.0,0.0,0.0,0.0,0.0,0.003921569,0.0,0.36078432,0.8745098,0.0627451,0.0,0.02745098,0.015686275,0.003921569,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.14901961,0.7176471,0.31764707,0.0,0.0,0.0,0.007843138,0.007843138,0.003921569,0.007843138,0.003921569,0.0,0.003921569,0.0,0.47058824,0.8862745,0.22352941,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.6784314,0.84705883,0.75686276,0.8352941,0.7176471,0.6431373,0.654902,0.38431373,0.0627451,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41960785,0.5529412,0.43529412,0.35686275,0.3529412,0.42352942,0.19607843,0.0],[0.0,0.0,0.7254902,0.8666667,0.8509804,0.8235294,0.7921569,0.87058824,0.78431374,0.80784315,0.7921569,0.8,0.42745098,0.105882354,0.047058824,0.06666667,0.23921569,0.53333336,0.7058824,0.8666667,0.7921569,0.88235295,0.8156863,0.8392157,0.8745098,0.9411765,0.63529414,0.0],[0.0,0.0,0.7176471,0.8627451,0.8039216,0.78431374,0.75686276,0.72156864,0.7411765,0.7137255,0.6784314,0.7607843,0.84313726,0.84705883,0.8039216,0.8117647,0.8117647,0.7647059,0.7254902,0.7607843,0.7607843,0.79607844,0.83137256,0.7176471,0.75686276,0.827451,0.6,0.0],[0.0,0.0,0.74509805,0.9137255,0.79607844,0.80784315,0.8392157,0.84705883,0.7647059,0.7176471,0.6901961,0.62352943,0.6862745,0.7411765,0.7921569,0.7647059,0.7294118,0.7294118,0.7137255,0.7294118,0.7647059,0.81960785,0.92156863,0.79607844,0.7921569,0.84313726,0.53333336,0.0],[0.0,0.0,0.7058824,0.89411765,0.7882353,0.7921569,0.7372549,0.7176471,0.69803923,0.7529412,0.7294118,0.70980394,0.70980394,0.69411767,0.8,0.8509804,0.6862745,0.7019608,0.72156864,0.6901961,0.6509804,0.6627451,0.6862745,0.7254902,0.59607846,0.79607844,0.41960785,0.0],[0.0,0.0,0.654902,0.92156863,0.78431374,0.8117647,0.8117647,0.81960785,0.8117647,0.79607844,0.7764706,0.7411765,0.7137255,0.69803923,0.7294118,0.7411765,0.69803923,0.7529412,0.77254903,0.7647059,0.7529412,0.7254902,0.72156864,0.8039216,0.73333335,1.0,0.23921569,0.0],[0.0,0.0,0.59607846,0.98039216,0.8156863,0.8392157,0.81960785,0.7921569,0.78431374,0.7921569,0.8039216,0.8,0.78431374,0.7529412,0.7764706,0.8,0.7647059,0.8039216,0.80784315,0.7764706,0.78039217,0.79607844,0.8392157,0.8117647,0.7019608,0.9764706,0.21568628,0.0],[0.0,0.0,0.49411765,1.0,0.827451,0.84313726,0.8235294,0.80784315,0.79607844,0.79607844,0.79607844,0.80784315,0.8,0.78039217,0.80784315,0.8117647,0.78431374,0.80784315,0.7882353,0.78039217,0.7921569,0.8156863,0.84313726,0.7647059,0.68235296,0.827451,0.05882353,0.0],[0.0,0.0,0.41960785,1.0,0.83137256,0.827451,0.8235294,0.827451,0.8156863,0.80784315,0.80784315,0.8117647,0.8156863,0.8156863,0.8509804,0.8392157,0.8039216,0.80784315,0.8,0.8156863,0.827451,0.827451,0.8627451,0.77254903,0.7137255,0.88235295,0.0,0.0],[0.0,0.0,0.24705882,0.9098039,0.827451,0.84705883,0.83137256,0.8392157,0.83137256,0.83137256,0.8392157,0.83137256,0.827451,0.83137256,0.8745098,0.85882354,0.8235294,0.8392157,0.8352941,0.83137256,0.827451,0.81960785,0.8509804,0.8156863,0.65882355,0.74509805,0.0,0.0],[0.0,0.0,0.05490196,1.0,0.85490197,0.8666667,0.84313726,0.85490197,0.85490197,0.8509804,0.84313726,0.8352941,0.827451,0.84313726,0.89411765,0.8627451,0.84705883,0.87058824,0.8509804,0.8509804,0.84705883,0.85490197,0.83137256,0.8352941,0.7411765,0.56078434,0.0,0.0],[0.0,0.0,0.0,0.9019608,0.8901961,0.8392157,0.8509804,0.85490197,0.84705883,0.8509804,0.8509804,0.8392157,0.84705883,0.87058824,0.8862745,0.85490197,0.85490197,0.8627451,0.85882354,0.85490197,0.8509804,0.85490197,0.8509804,0.84313726,0.84705883,0.44313726,0.0,0.0],[0.0,0.0,0.0,0.69411767,0.9137255,0.84705883,0.8745098,0.87058824,0.8352941,0.8352941,0.84313726,0.83137256,0.8627451,0.8862745,0.8666667,0.8627451,0.8666667,0.85490197,0.87058824,0.8627451,0.8627451,0.8666667,0.8509804,0.83137256,0.85882354,0.20392157,0.0,0.0],[0.0,0.0,0.0,0.039215688,0.8745098,0.87058824,0.8627451,0.827451,0.8352941,0.84313726,0.8627451,0.87058824,0.8862745,0.8862745,0.8666667,0.8745098,0.87058824,0.87058824,0.8666667,0.85490197,0.8627451,0.85882354,0.8627451,0.8745098,0.68235296,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.49411765,0.94509804,0.8156863,0.8235294,0.8392157,0.8392157,0.84705883,0.84705883,0.8627451,0.8627451,0.8392157,0.8352941,0.83137256,0.83137256,0.8235294,0.84313726,0.8509804,0.85490197,0.84313726,0.9254902,0.16470589,0.0,0.003921569,0.0],[0.0,0.0,0.0,0.0,0.0,0.7372549,0.92941177,0.9019608,0.9137255,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.90588236,0.8980392,0.9372549,0.6313726,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.24705882,0.37254903,0.42352942,0.4117647,0.39215687,0.40392157,0.40784314,0.4117647,0.4,0.39215687,0.38431373,0.38039216,0.3764706,0.3529412,0.3137255,0.3254902,0.23529412,0.0,0.0,0.0,0.0,0.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(0, 0, 0)\"],[0.09090909090909091,\"rgb(16, 16, 16)\"],[0.18181818181818182,\"rgb(38, 38, 38)\"],[0.2727272727272727,\"rgb(59, 59, 59)\"],[0.36363636363636365,\"rgb(81, 80, 80)\"],[0.45454545454545453,\"rgb(102, 101, 101)\"],[0.5454545454545454,\"rgb(124, 123, 122)\"],[0.6363636363636364,\"rgb(146, 146, 145)\"],[0.7272727272727273,\"rgb(171, 171, 170)\"],[0.8181818181818182,\"rgb(197, 197, 195)\"],[0.9090909090909091,\"rgb(224, 224, 223)\"],[1.0,\"rgb(254, 254, 253)\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3f0e1dd6-c869-4a12-8d22-f2b9398178ca');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 6. Create model\n",
        "# -----------------------------\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FirstNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    # Define the components of the model\n",
        "    super(FirstNet, self).__init__()\n",
        "    # Function to flatten our image\n",
        "    self.flatten = nn.Flatten()\n",
        "    # Create the sequence of our network\n",
        "    self.linear_relu_model = nn.Sequential(\n",
        "        # Add a linear output layer with 10 perceptrons\n",
        "        nn.LazyLinear(10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Construct the sequencing of the model here\n",
        "    x = self.flatten(x)\n",
        "    # Pass flattened images through our sequence\n",
        "    output = self.linear_relu_model(x)\n",
        "\n",
        "    # Return the evaluations of our ten classes as a 10-dimensional vector\n",
        "    return output\n",
        "\n",
        "  # Create an instance of our model\n",
        "model = FirstNet()"
      ],
      "metadata": {
        "id": "mvLXISyrBzSo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 7. Prepare to Train\n",
        "# -----------------------------\n",
        "# Define training parameters\n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "# Define the loss function, for multiclass problems\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "b39RyfaWED2H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build our optimizer with the parameters from the model we defined, and the learning rate we selected\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "J80-Tx5YEVOm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  # Set the model to training mode, important for batch normalization & dropout layers\n",
        "  model.train()\n",
        "  # Loop over batches via the dataloader\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation and looking for improved gradients\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Zeroing out the gradient (otherwise they are summed) in prep for next round\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Print progress update every few loops\n",
        "    if batch % 10 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "CJylH5L8Ejus"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 8. Prepare to Train AND Test the Model\n",
        "# -----------------------------\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  # Evaluating the model with torch.no_grad()\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) ==y).type(torch.float).sum().item()\n",
        "\n",
        "  # Printing some output after a testing round\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "-d7Z2kzCFlBW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 9. Train the Model\n",
        "# -----------------------------\n",
        "# Repeat the training process for each epoch\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1} \\n---------------------------\")\n",
        "  train_loop(train_loader, model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkjb-UEdHrDF",
        "outputId": "d2533b9c-4b34-4b02-9b97-cabcb0a95aec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            "---------------------------\n",
            "loss: 2.307655  [   64/60000]\n",
            "loss: 2.121762  [  704/60000]\n",
            "loss: 1.981479  [ 1344/60000]\n",
            "loss: 1.795505  [ 1984/60000]\n",
            "loss: 1.699080  [ 2624/60000]\n",
            "loss: 1.682289  [ 3264/60000]\n",
            "loss: 1.567694  [ 3904/60000]\n",
            "loss: 1.559464  [ 4544/60000]\n",
            "loss: 1.460485  [ 5184/60000]\n",
            "loss: 1.394568  [ 5824/60000]\n",
            "loss: 1.357782  [ 6464/60000]\n",
            "loss: 1.420395  [ 7104/60000]\n",
            "loss: 1.327754  [ 7744/60000]\n",
            "loss: 1.259997  [ 8384/60000]\n",
            "loss: 1.223877  [ 9024/60000]\n",
            "loss: 1.204618  [ 9664/60000]\n",
            "loss: 1.063499  [10304/60000]\n",
            "loss: 1.194144  [10944/60000]\n",
            "loss: 1.033257  [11584/60000]\n",
            "loss: 1.083131  [12224/60000]\n",
            "loss: 1.069833  [12864/60000]\n",
            "loss: 1.051857  [13504/60000]\n",
            "loss: 1.077788  [14144/60000]\n",
            "loss: 1.025258  [14784/60000]\n",
            "loss: 0.982967  [15424/60000]\n",
            "loss: 0.934557  [16064/60000]\n",
            "loss: 0.978111  [16704/60000]\n",
            "loss: 1.052807  [17344/60000]\n",
            "loss: 0.966036  [17984/60000]\n",
            "loss: 0.944122  [18624/60000]\n",
            "loss: 0.948546  [19264/60000]\n",
            "loss: 1.026074  [19904/60000]\n",
            "loss: 0.926351  [20544/60000]\n",
            "loss: 1.032210  [21184/60000]\n",
            "loss: 0.904293  [21824/60000]\n",
            "loss: 0.834947  [22464/60000]\n",
            "loss: 0.861226  [23104/60000]\n",
            "loss: 0.920327  [23744/60000]\n",
            "loss: 0.854928  [24384/60000]\n",
            "loss: 0.928019  [25024/60000]\n",
            "loss: 0.916236  [25664/60000]\n",
            "loss: 0.911491  [26304/60000]\n",
            "loss: 0.834943  [26944/60000]\n",
            "loss: 0.935807  [27584/60000]\n",
            "loss: 0.805272  [28224/60000]\n",
            "loss: 0.821740  [28864/60000]\n",
            "loss: 0.720554  [29504/60000]\n",
            "loss: 1.001723  [30144/60000]\n",
            "loss: 0.816702  [30784/60000]\n",
            "loss: 0.825331  [31424/60000]\n",
            "loss: 0.884948  [32064/60000]\n",
            "loss: 0.917898  [32704/60000]\n",
            "loss: 0.800262  [33344/60000]\n",
            "loss: 0.769119  [33984/60000]\n",
            "loss: 0.711190  [34624/60000]\n",
            "loss: 0.579548  [35264/60000]\n",
            "loss: 0.719169  [35904/60000]\n",
            "loss: 0.777403  [36544/60000]\n",
            "loss: 0.622610  [37184/60000]\n",
            "loss: 0.763515  [37824/60000]\n",
            "loss: 0.923890  [38464/60000]\n",
            "loss: 0.882834  [39104/60000]\n",
            "loss: 0.837564  [39744/60000]\n",
            "loss: 0.690771  [40384/60000]\n",
            "loss: 0.852756  [41024/60000]\n",
            "loss: 0.775107  [41664/60000]\n",
            "loss: 0.817996  [42304/60000]\n",
            "loss: 0.904126  [42944/60000]\n",
            "loss: 0.982677  [43584/60000]\n",
            "loss: 0.890292  [44224/60000]\n",
            "loss: 0.646765  [44864/60000]\n",
            "loss: 0.694942  [45504/60000]\n",
            "loss: 0.813472  [46144/60000]\n",
            "loss: 0.790532  [46784/60000]\n",
            "loss: 0.733715  [47424/60000]\n",
            "loss: 0.670245  [48064/60000]\n",
            "loss: 0.829138  [48704/60000]\n",
            "loss: 0.700744  [49344/60000]\n",
            "loss: 0.766821  [49984/60000]\n",
            "loss: 0.716195  [50624/60000]\n",
            "loss: 0.765987  [51264/60000]\n",
            "loss: 0.635758  [51904/60000]\n",
            "loss: 0.736381  [52544/60000]\n",
            "loss: 0.804000  [53184/60000]\n",
            "loss: 0.626101  [53824/60000]\n",
            "loss: 0.806633  [54464/60000]\n",
            "loss: 0.703483  [55104/60000]\n",
            "loss: 0.750561  [55744/60000]\n",
            "loss: 0.638330  [56384/60000]\n",
            "loss: 0.672318  [57024/60000]\n",
            "loss: 0.654066  [57664/60000]\n",
            "loss: 0.811841  [58304/60000]\n",
            "loss: 0.694190  [58944/60000]\n",
            "loss: 0.908689  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.8%, Avg loss: 0.738350 \n",
            "\n",
            "Epoch 2 \n",
            "---------------------------\n",
            "loss: 0.634851  [   64/60000]\n",
            "loss: 0.767271  [  704/60000]\n",
            "loss: 0.809271  [ 1344/60000]\n",
            "loss: 0.918017  [ 1984/60000]\n",
            "loss: 0.580103  [ 2624/60000]\n",
            "loss: 0.649948  [ 3264/60000]\n",
            "loss: 0.783895  [ 3904/60000]\n",
            "loss: 0.784546  [ 4544/60000]\n",
            "loss: 0.793218  [ 5184/60000]\n",
            "loss: 0.659420  [ 5824/60000]\n",
            "loss: 0.601439  [ 6464/60000]\n",
            "loss: 0.633178  [ 7104/60000]\n",
            "loss: 0.575656  [ 7744/60000]\n",
            "loss: 0.694204  [ 8384/60000]\n",
            "loss: 0.744646  [ 9024/60000]\n",
            "loss: 0.694340  [ 9664/60000]\n",
            "loss: 0.689365  [10304/60000]\n",
            "loss: 0.607263  [10944/60000]\n",
            "loss: 0.659469  [11584/60000]\n",
            "loss: 0.718075  [12224/60000]\n",
            "loss: 0.633420  [12864/60000]\n",
            "loss: 0.662585  [13504/60000]\n",
            "loss: 0.614873  [14144/60000]\n",
            "loss: 0.752199  [14784/60000]\n",
            "loss: 0.717148  [15424/60000]\n",
            "loss: 0.646531  [16064/60000]\n",
            "loss: 0.638268  [16704/60000]\n",
            "loss: 0.784450  [17344/60000]\n",
            "loss: 0.712566  [17984/60000]\n",
            "loss: 0.873462  [18624/60000]\n",
            "loss: 0.678668  [19264/60000]\n",
            "loss: 0.683667  [19904/60000]\n",
            "loss: 0.771795  [20544/60000]\n",
            "loss: 0.634332  [21184/60000]\n",
            "loss: 0.619131  [21824/60000]\n",
            "loss: 0.606734  [22464/60000]\n",
            "loss: 0.634374  [23104/60000]\n",
            "loss: 0.630003  [23744/60000]\n",
            "loss: 0.535395  [24384/60000]\n",
            "loss: 0.746401  [25024/60000]\n",
            "loss: 0.653053  [25664/60000]\n",
            "loss: 0.626341  [26304/60000]\n",
            "loss: 0.621512  [26944/60000]\n",
            "loss: 0.610272  [27584/60000]\n",
            "loss: 0.811586  [28224/60000]\n",
            "loss: 0.735924  [28864/60000]\n",
            "loss: 0.507210  [29504/60000]\n",
            "loss: 0.674615  [30144/60000]\n",
            "loss: 0.495563  [30784/60000]\n",
            "loss: 0.650334  [31424/60000]\n",
            "loss: 0.551465  [32064/60000]\n",
            "loss: 0.719712  [32704/60000]\n",
            "loss: 0.656190  [33344/60000]\n",
            "loss: 0.671427  [33984/60000]\n",
            "loss: 0.611177  [34624/60000]\n",
            "loss: 0.604336  [35264/60000]\n",
            "loss: 0.674185  [35904/60000]\n",
            "loss: 0.860284  [36544/60000]\n",
            "loss: 0.673360  [37184/60000]\n",
            "loss: 0.729476  [37824/60000]\n",
            "loss: 0.611333  [38464/60000]\n",
            "loss: 0.667542  [39104/60000]\n",
            "loss: 0.716069  [39744/60000]\n",
            "loss: 0.770749  [40384/60000]\n",
            "loss: 0.533274  [41024/60000]\n",
            "loss: 0.653600  [41664/60000]\n",
            "loss: 0.570087  [42304/60000]\n",
            "loss: 0.542867  [42944/60000]\n",
            "loss: 0.553524  [43584/60000]\n",
            "loss: 0.664171  [44224/60000]\n",
            "loss: 0.645539  [44864/60000]\n",
            "loss: 0.626657  [45504/60000]\n",
            "loss: 0.664128  [46144/60000]\n",
            "loss: 0.790349  [46784/60000]\n",
            "loss: 0.581675  [47424/60000]\n",
            "loss: 0.597505  [48064/60000]\n",
            "loss: 0.697034  [48704/60000]\n",
            "loss: 0.702151  [49344/60000]\n",
            "loss: 0.746696  [49984/60000]\n",
            "loss: 0.635333  [50624/60000]\n",
            "loss: 0.606766  [51264/60000]\n",
            "loss: 0.647570  [51904/60000]\n",
            "loss: 0.541703  [52544/60000]\n",
            "loss: 0.641080  [53184/60000]\n",
            "loss: 0.701279  [53824/60000]\n",
            "loss: 0.626481  [54464/60000]\n",
            "loss: 0.513037  [55104/60000]\n",
            "loss: 0.629562  [55744/60000]\n",
            "loss: 0.652232  [56384/60000]\n",
            "loss: 0.596383  [57024/60000]\n",
            "loss: 0.602406  [57664/60000]\n",
            "loss: 0.489771  [58304/60000]\n",
            "loss: 0.494834  [58944/60000]\n",
            "loss: 0.556324  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.6%, Avg loss: 0.647044 \n",
            "\n",
            "Epoch 3 \n",
            "---------------------------\n",
            "loss: 0.732757  [   64/60000]\n",
            "loss: 0.605105  [  704/60000]\n",
            "loss: 0.593711  [ 1344/60000]\n",
            "loss: 0.838393  [ 1984/60000]\n",
            "loss: 0.801598  [ 2624/60000]\n",
            "loss: 0.800374  [ 3264/60000]\n",
            "loss: 0.511795  [ 3904/60000]\n",
            "loss: 0.540852  [ 4544/60000]\n",
            "loss: 0.698642  [ 5184/60000]\n",
            "loss: 0.560808  [ 5824/60000]\n",
            "loss: 0.877145  [ 6464/60000]\n",
            "loss: 0.588570  [ 7104/60000]\n",
            "loss: 0.815480  [ 7744/60000]\n",
            "loss: 0.570417  [ 8384/60000]\n",
            "loss: 0.642077  [ 9024/60000]\n",
            "loss: 0.719004  [ 9664/60000]\n",
            "loss: 0.636787  [10304/60000]\n",
            "loss: 0.592704  [10944/60000]\n",
            "loss: 0.750206  [11584/60000]\n",
            "loss: 0.556968  [12224/60000]\n",
            "loss: 0.644444  [12864/60000]\n",
            "loss: 0.589876  [13504/60000]\n",
            "loss: 0.472062  [14144/60000]\n",
            "loss: 0.624700  [14784/60000]\n",
            "loss: 0.634657  [15424/60000]\n",
            "loss: 0.669209  [16064/60000]\n",
            "loss: 0.655370  [16704/60000]\n",
            "loss: 0.667048  [17344/60000]\n",
            "loss: 0.528216  [17984/60000]\n",
            "loss: 0.680168  [18624/60000]\n",
            "loss: 0.711348  [19264/60000]\n",
            "loss: 0.594765  [19904/60000]\n",
            "loss: 0.689446  [20544/60000]\n",
            "loss: 0.649381  [21184/60000]\n",
            "loss: 0.670264  [21824/60000]\n",
            "loss: 0.796259  [22464/60000]\n",
            "loss: 0.599780  [23104/60000]\n",
            "loss: 0.580399  [23744/60000]\n",
            "loss: 0.523651  [24384/60000]\n",
            "loss: 0.714172  [25024/60000]\n",
            "loss: 0.651441  [25664/60000]\n",
            "loss: 0.657612  [26304/60000]\n",
            "loss: 0.529857  [26944/60000]\n",
            "loss: 0.490298  [27584/60000]\n",
            "loss: 0.585669  [28224/60000]\n",
            "loss: 0.864705  [28864/60000]\n",
            "loss: 0.570275  [29504/60000]\n",
            "loss: 0.502887  [30144/60000]\n",
            "loss: 0.572429  [30784/60000]\n",
            "loss: 0.643508  [31424/60000]\n",
            "loss: 0.610946  [32064/60000]\n",
            "loss: 0.643886  [32704/60000]\n",
            "loss: 0.542989  [33344/60000]\n",
            "loss: 0.520444  [33984/60000]\n",
            "loss: 0.576426  [34624/60000]\n",
            "loss: 0.446923  [35264/60000]\n",
            "loss: 0.657068  [35904/60000]\n",
            "loss: 0.595287  [36544/60000]\n",
            "loss: 0.686350  [37184/60000]\n",
            "loss: 0.557306  [37824/60000]\n",
            "loss: 0.467901  [38464/60000]\n",
            "loss: 0.578255  [39104/60000]\n",
            "loss: 0.542786  [39744/60000]\n",
            "loss: 0.539010  [40384/60000]\n",
            "loss: 0.714040  [41024/60000]\n",
            "loss: 0.787000  [41664/60000]\n",
            "loss: 0.551868  [42304/60000]\n",
            "loss: 0.464409  [42944/60000]\n",
            "loss: 0.726780  [43584/60000]\n",
            "loss: 0.572417  [44224/60000]\n",
            "loss: 0.607454  [44864/60000]\n",
            "loss: 0.657610  [45504/60000]\n",
            "loss: 0.620159  [46144/60000]\n",
            "loss: 0.590227  [46784/60000]\n",
            "loss: 0.464944  [47424/60000]\n",
            "loss: 0.837277  [48064/60000]\n",
            "loss: 0.600088  [48704/60000]\n",
            "loss: 0.547198  [49344/60000]\n",
            "loss: 0.607625  [49984/60000]\n",
            "loss: 0.640040  [50624/60000]\n",
            "loss: 0.706931  [51264/60000]\n",
            "loss: 0.520458  [51904/60000]\n",
            "loss: 0.618940  [52544/60000]\n",
            "loss: 0.586887  [53184/60000]\n",
            "loss: 0.529664  [53824/60000]\n",
            "loss: 0.540199  [54464/60000]\n",
            "loss: 0.615515  [55104/60000]\n",
            "loss: 0.670539  [55744/60000]\n",
            "loss: 0.561820  [56384/60000]\n",
            "loss: 0.666041  [57024/60000]\n",
            "loss: 0.463386  [57664/60000]\n",
            "loss: 0.522790  [58304/60000]\n",
            "loss: 0.589872  [58944/60000]\n",
            "loss: 0.462759  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.7%, Avg loss: 0.607861 \n",
            "\n",
            "Epoch 4 \n",
            "---------------------------\n",
            "loss: 0.690216  [   64/60000]\n",
            "loss: 0.473957  [  704/60000]\n",
            "loss: 0.588920  [ 1344/60000]\n",
            "loss: 0.576804  [ 1984/60000]\n",
            "loss: 0.509369  [ 2624/60000]\n",
            "loss: 0.578522  [ 3264/60000]\n",
            "loss: 0.635274  [ 3904/60000]\n",
            "loss: 0.542982  [ 4544/60000]\n",
            "loss: 0.612896  [ 5184/60000]\n",
            "loss: 0.572872  [ 5824/60000]\n",
            "loss: 0.704133  [ 6464/60000]\n",
            "loss: 0.414005  [ 7104/60000]\n",
            "loss: 0.588618  [ 7744/60000]\n",
            "loss: 0.449706  [ 8384/60000]\n",
            "loss: 0.589986  [ 9024/60000]\n",
            "loss: 0.567620  [ 9664/60000]\n",
            "loss: 0.439640  [10304/60000]\n",
            "loss: 0.587655  [10944/60000]\n",
            "loss: 0.582973  [11584/60000]\n",
            "loss: 0.653943  [12224/60000]\n",
            "loss: 0.603691  [12864/60000]\n",
            "loss: 0.428155  [13504/60000]\n",
            "loss: 0.645499  [14144/60000]\n",
            "loss: 0.652307  [14784/60000]\n",
            "loss: 0.523113  [15424/60000]\n",
            "loss: 0.579023  [16064/60000]\n",
            "loss: 0.512397  [16704/60000]\n",
            "loss: 0.639747  [17344/60000]\n",
            "loss: 0.621676  [17984/60000]\n",
            "loss: 0.531999  [18624/60000]\n",
            "loss: 0.512118  [19264/60000]\n",
            "loss: 0.545111  [19904/60000]\n",
            "loss: 0.545686  [20544/60000]\n",
            "loss: 0.622046  [21184/60000]\n",
            "loss: 0.534898  [21824/60000]\n",
            "loss: 0.685336  [22464/60000]\n",
            "loss: 0.701106  [23104/60000]\n",
            "loss: 0.535354  [23744/60000]\n",
            "loss: 0.442968  [24384/60000]\n",
            "loss: 0.464016  [25024/60000]\n",
            "loss: 0.741017  [25664/60000]\n",
            "loss: 0.541624  [26304/60000]\n",
            "loss: 0.577233  [26944/60000]\n",
            "loss: 0.571396  [27584/60000]\n",
            "loss: 0.597615  [28224/60000]\n",
            "loss: 0.493424  [28864/60000]\n",
            "loss: 0.566461  [29504/60000]\n",
            "loss: 0.676826  [30144/60000]\n",
            "loss: 0.460985  [30784/60000]\n",
            "loss: 0.531566  [31424/60000]\n",
            "loss: 0.628417  [32064/60000]\n",
            "loss: 0.632906  [32704/60000]\n",
            "loss: 0.437432  [33344/60000]\n",
            "loss: 0.460795  [33984/60000]\n",
            "loss: 0.579620  [34624/60000]\n",
            "loss: 0.498536  [35264/60000]\n",
            "loss: 0.581091  [35904/60000]\n",
            "loss: 0.699958  [36544/60000]\n",
            "loss: 0.470633  [37184/60000]\n",
            "loss: 0.566064  [37824/60000]\n",
            "loss: 0.474381  [38464/60000]\n",
            "loss: 0.560601  [39104/60000]\n",
            "loss: 0.615076  [39744/60000]\n",
            "loss: 0.808625  [40384/60000]\n",
            "loss: 0.629284  [41024/60000]\n",
            "loss: 0.523921  [41664/60000]\n",
            "loss: 0.472744  [42304/60000]\n",
            "loss: 0.568478  [42944/60000]\n",
            "loss: 0.594240  [43584/60000]\n",
            "loss: 0.544453  [44224/60000]\n",
            "loss: 0.744841  [44864/60000]\n",
            "loss: 0.575698  [45504/60000]\n",
            "loss: 0.500032  [46144/60000]\n",
            "loss: 0.543904  [46784/60000]\n",
            "loss: 0.622609  [47424/60000]\n",
            "loss: 0.493253  [48064/60000]\n",
            "loss: 0.456489  [48704/60000]\n",
            "loss: 0.550144  [49344/60000]\n",
            "loss: 0.457832  [49984/60000]\n",
            "loss: 0.717560  [50624/60000]\n",
            "loss: 0.461854  [51264/60000]\n",
            "loss: 0.669682  [51904/60000]\n",
            "loss: 0.519207  [52544/60000]\n",
            "loss: 0.534537  [53184/60000]\n",
            "loss: 0.667539  [53824/60000]\n",
            "loss: 0.454902  [54464/60000]\n",
            "loss: 0.411542  [55104/60000]\n",
            "loss: 0.661656  [55744/60000]\n",
            "loss: 0.643713  [56384/60000]\n",
            "loss: 0.496538  [57024/60000]\n",
            "loss: 0.442735  [57664/60000]\n",
            "loss: 0.528159  [58304/60000]\n",
            "loss: 0.475847  [58944/60000]\n",
            "loss: 0.353738  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.578247 \n",
            "\n",
            "Epoch 5 \n",
            "---------------------------\n",
            "loss: 0.596514  [   64/60000]\n",
            "loss: 0.530201  [  704/60000]\n",
            "loss: 0.532053  [ 1344/60000]\n",
            "loss: 0.568836  [ 1984/60000]\n",
            "loss: 0.550571  [ 2624/60000]\n",
            "loss: 0.557201  [ 3264/60000]\n",
            "loss: 0.593215  [ 3904/60000]\n",
            "loss: 0.531999  [ 4544/60000]\n",
            "loss: 0.604359  [ 5184/60000]\n",
            "loss: 0.604534  [ 5824/60000]\n",
            "loss: 0.485292  [ 6464/60000]\n",
            "loss: 0.519675  [ 7104/60000]\n",
            "loss: 0.555975  [ 7744/60000]\n",
            "loss: 0.610121  [ 8384/60000]\n",
            "loss: 0.565115  [ 9024/60000]\n",
            "loss: 0.498716  [ 9664/60000]\n",
            "loss: 0.539457  [10304/60000]\n",
            "loss: 0.587335  [10944/60000]\n",
            "loss: 0.520816  [11584/60000]\n",
            "loss: 0.545473  [12224/60000]\n",
            "loss: 0.461002  [12864/60000]\n",
            "loss: 0.587167  [13504/60000]\n",
            "loss: 0.580155  [14144/60000]\n",
            "loss: 0.444056  [14784/60000]\n",
            "loss: 0.661734  [15424/60000]\n",
            "loss: 0.511206  [16064/60000]\n",
            "loss: 0.403195  [16704/60000]\n",
            "loss: 0.535155  [17344/60000]\n",
            "loss: 0.512925  [17984/60000]\n",
            "loss: 0.577334  [18624/60000]\n",
            "loss: 0.540180  [19264/60000]\n",
            "loss: 0.469926  [19904/60000]\n",
            "loss: 0.667330  [20544/60000]\n",
            "loss: 0.530501  [21184/60000]\n",
            "loss: 0.561595  [21824/60000]\n",
            "loss: 0.681406  [22464/60000]\n",
            "loss: 0.674636  [23104/60000]\n",
            "loss: 0.651907  [23744/60000]\n",
            "loss: 0.485685  [24384/60000]\n",
            "loss: 0.765835  [25024/60000]\n",
            "loss: 0.606818  [25664/60000]\n",
            "loss: 0.773143  [26304/60000]\n",
            "loss: 0.557985  [26944/60000]\n",
            "loss: 0.455506  [27584/60000]\n",
            "loss: 0.682417  [28224/60000]\n",
            "loss: 0.590945  [28864/60000]\n",
            "loss: 0.593829  [29504/60000]\n",
            "loss: 0.593161  [30144/60000]\n",
            "loss: 0.524317  [30784/60000]\n",
            "loss: 0.410949  [31424/60000]\n",
            "loss: 0.448002  [32064/60000]\n",
            "loss: 0.721405  [32704/60000]\n",
            "loss: 0.648413  [33344/60000]\n",
            "loss: 0.470742  [33984/60000]\n",
            "loss: 0.635090  [34624/60000]\n",
            "loss: 0.704218  [35264/60000]\n",
            "loss: 0.511619  [35904/60000]\n",
            "loss: 0.436814  [36544/60000]\n",
            "loss: 0.469619  [37184/60000]\n",
            "loss: 0.520504  [37824/60000]\n",
            "loss: 0.581328  [38464/60000]\n",
            "loss: 0.477816  [39104/60000]\n",
            "loss: 0.398696  [39744/60000]\n",
            "loss: 0.681978  [40384/60000]\n",
            "loss: 0.468949  [41024/60000]\n",
            "loss: 0.498977  [41664/60000]\n",
            "loss: 0.517767  [42304/60000]\n",
            "loss: 0.632675  [42944/60000]\n",
            "loss: 0.450199  [43584/60000]\n",
            "loss: 0.608870  [44224/60000]\n",
            "loss: 0.352072  [44864/60000]\n",
            "loss: 0.633180  [45504/60000]\n",
            "loss: 0.579931  [46144/60000]\n",
            "loss: 0.588739  [46784/60000]\n",
            "loss: 0.590119  [47424/60000]\n",
            "loss: 0.901626  [48064/60000]\n",
            "loss: 0.459940  [48704/60000]\n",
            "loss: 0.659404  [49344/60000]\n",
            "loss: 0.443412  [49984/60000]\n",
            "loss: 0.462758  [50624/60000]\n",
            "loss: 0.630543  [51264/60000]\n",
            "loss: 0.531164  [51904/60000]\n",
            "loss: 0.564781  [52544/60000]\n",
            "loss: 0.524650  [53184/60000]\n",
            "loss: 0.516694  [53824/60000]\n",
            "loss: 0.374494  [54464/60000]\n",
            "loss: 0.383806  [55104/60000]\n",
            "loss: 0.494503  [55744/60000]\n",
            "loss: 0.502514  [56384/60000]\n",
            "loss: 0.482705  [57024/60000]\n",
            "loss: 0.451807  [57664/60000]\n",
            "loss: 0.493272  [58304/60000]\n",
            "loss: 0.537959  [58944/60000]\n",
            "loss: 0.462531  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.561176 \n",
            "\n",
            "Epoch 6 \n",
            "---------------------------\n",
            "loss: 0.499004  [   64/60000]\n",
            "loss: 0.523395  [  704/60000]\n",
            "loss: 0.774686  [ 1344/60000]\n",
            "loss: 0.437747  [ 1984/60000]\n",
            "loss: 0.620738  [ 2624/60000]\n",
            "loss: 0.414496  [ 3264/60000]\n",
            "loss: 0.505844  [ 3904/60000]\n",
            "loss: 0.597989  [ 4544/60000]\n",
            "loss: 0.481427  [ 5184/60000]\n",
            "loss: 0.431988  [ 5824/60000]\n",
            "loss: 0.647846  [ 6464/60000]\n",
            "loss: 0.482100  [ 7104/60000]\n",
            "loss: 0.497628  [ 7744/60000]\n",
            "loss: 0.521947  [ 8384/60000]\n",
            "loss: 0.461467  [ 9024/60000]\n",
            "loss: 0.378697  [ 9664/60000]\n",
            "loss: 0.499273  [10304/60000]\n",
            "loss: 0.561319  [10944/60000]\n",
            "loss: 0.573295  [11584/60000]\n",
            "loss: 0.543548  [12224/60000]\n",
            "loss: 0.501697  [12864/60000]\n",
            "loss: 0.322395  [13504/60000]\n",
            "loss: 0.465775  [14144/60000]\n",
            "loss: 0.468407  [14784/60000]\n",
            "loss: 0.485958  [15424/60000]\n",
            "loss: 0.425490  [16064/60000]\n",
            "loss: 0.612891  [16704/60000]\n",
            "loss: 0.419754  [17344/60000]\n",
            "loss: 0.433866  [17984/60000]\n",
            "loss: 0.563811  [18624/60000]\n",
            "loss: 0.588600  [19264/60000]\n",
            "loss: 0.464966  [19904/60000]\n",
            "loss: 0.638777  [20544/60000]\n",
            "loss: 0.604129  [21184/60000]\n",
            "loss: 0.475455  [21824/60000]\n",
            "loss: 0.424655  [22464/60000]\n",
            "loss: 0.566012  [23104/60000]\n",
            "loss: 0.397817  [23744/60000]\n",
            "loss: 0.564370  [24384/60000]\n",
            "loss: 0.731281  [25024/60000]\n",
            "loss: 0.748790  [25664/60000]\n",
            "loss: 0.448554  [26304/60000]\n",
            "loss: 0.512709  [26944/60000]\n",
            "loss: 0.771705  [27584/60000]\n",
            "loss: 0.573496  [28224/60000]\n",
            "loss: 0.539191  [28864/60000]\n",
            "loss: 0.414182  [29504/60000]\n",
            "loss: 0.502398  [30144/60000]\n",
            "loss: 0.586228  [30784/60000]\n",
            "loss: 0.529337  [31424/60000]\n",
            "loss: 0.609071  [32064/60000]\n",
            "loss: 0.516742  [32704/60000]\n",
            "loss: 0.568979  [33344/60000]\n",
            "loss: 0.544666  [33984/60000]\n",
            "loss: 0.517526  [34624/60000]\n",
            "loss: 0.469259  [35264/60000]\n",
            "loss: 0.466326  [35904/60000]\n",
            "loss: 0.685878  [36544/60000]\n",
            "loss: 0.604613  [37184/60000]\n",
            "loss: 0.458181  [37824/60000]\n",
            "loss: 0.542442  [38464/60000]\n",
            "loss: 0.518608  [39104/60000]\n",
            "loss: 0.451845  [39744/60000]\n",
            "loss: 0.586245  [40384/60000]\n",
            "loss: 0.535615  [41024/60000]\n",
            "loss: 0.565974  [41664/60000]\n",
            "loss: 0.578663  [42304/60000]\n",
            "loss: 0.439251  [42944/60000]\n",
            "loss: 0.399944  [43584/60000]\n",
            "loss: 0.504867  [44224/60000]\n",
            "loss: 0.494598  [44864/60000]\n",
            "loss: 0.448213  [45504/60000]\n",
            "loss: 0.507577  [46144/60000]\n",
            "loss: 0.496999  [46784/60000]\n",
            "loss: 0.494639  [47424/60000]\n",
            "loss: 0.536104  [48064/60000]\n",
            "loss: 0.462708  [48704/60000]\n",
            "loss: 0.550782  [49344/60000]\n",
            "loss: 0.664085  [49984/60000]\n",
            "loss: 0.534866  [50624/60000]\n",
            "loss: 0.333496  [51264/60000]\n",
            "loss: 0.427711  [51904/60000]\n",
            "loss: 0.495030  [52544/60000]\n",
            "loss: 0.460599  [53184/60000]\n",
            "loss: 0.523198  [53824/60000]\n",
            "loss: 0.651936  [54464/60000]\n",
            "loss: 0.527154  [55104/60000]\n",
            "loss: 0.459295  [55744/60000]\n",
            "loss: 0.527576  [56384/60000]\n",
            "loss: 0.456484  [57024/60000]\n",
            "loss: 0.368515  [57664/60000]\n",
            "loss: 0.604597  [58304/60000]\n",
            "loss: 0.421254  [58944/60000]\n",
            "loss: 0.862561  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.548524 \n",
            "\n",
            "Epoch 7 \n",
            "---------------------------\n",
            "loss: 0.601586  [   64/60000]\n",
            "loss: 0.544090  [  704/60000]\n",
            "loss: 0.452924  [ 1344/60000]\n",
            "loss: 0.477870  [ 1984/60000]\n",
            "loss: 0.520185  [ 2624/60000]\n",
            "loss: 0.576573  [ 3264/60000]\n",
            "loss: 0.454653  [ 3904/60000]\n",
            "loss: 0.337530  [ 4544/60000]\n",
            "loss: 0.492980  [ 5184/60000]\n",
            "loss: 0.482954  [ 5824/60000]\n",
            "loss: 0.542691  [ 6464/60000]\n",
            "loss: 0.504665  [ 7104/60000]\n",
            "loss: 0.640646  [ 7744/60000]\n",
            "loss: 0.446517  [ 8384/60000]\n",
            "loss: 0.465648  [ 9024/60000]\n",
            "loss: 0.529592  [ 9664/60000]\n",
            "loss: 0.528819  [10304/60000]\n",
            "loss: 0.471209  [10944/60000]\n",
            "loss: 0.384050  [11584/60000]\n",
            "loss: 0.482984  [12224/60000]\n",
            "loss: 0.500891  [12864/60000]\n",
            "loss: 0.756861  [13504/60000]\n",
            "loss: 0.517368  [14144/60000]\n",
            "loss: 0.371928  [14784/60000]\n",
            "loss: 0.402623  [15424/60000]\n",
            "loss: 0.501828  [16064/60000]\n",
            "loss: 0.446439  [16704/60000]\n",
            "loss: 0.663739  [17344/60000]\n",
            "loss: 0.486988  [17984/60000]\n",
            "loss: 0.667756  [18624/60000]\n",
            "loss: 0.546060  [19264/60000]\n",
            "loss: 0.520086  [19904/60000]\n",
            "loss: 0.575046  [20544/60000]\n",
            "loss: 0.530978  [21184/60000]\n",
            "loss: 0.535633  [21824/60000]\n",
            "loss: 0.617460  [22464/60000]\n",
            "loss: 0.458935  [23104/60000]\n",
            "loss: 0.452326  [23744/60000]\n",
            "loss: 0.661811  [24384/60000]\n",
            "loss: 0.481788  [25024/60000]\n",
            "loss: 0.511159  [25664/60000]\n",
            "loss: 0.583667  [26304/60000]\n",
            "loss: 0.416442  [26944/60000]\n",
            "loss: 0.281226  [27584/60000]\n",
            "loss: 0.468914  [28224/60000]\n",
            "loss: 0.503060  [28864/60000]\n",
            "loss: 0.498233  [29504/60000]\n",
            "loss: 0.530458  [30144/60000]\n",
            "loss: 0.490115  [30784/60000]\n",
            "loss: 0.621195  [31424/60000]\n",
            "loss: 0.527679  [32064/60000]\n",
            "loss: 0.400514  [32704/60000]\n",
            "loss: 0.408791  [33344/60000]\n",
            "loss: 0.536315  [33984/60000]\n",
            "loss: 0.586127  [34624/60000]\n",
            "loss: 0.523086  [35264/60000]\n",
            "loss: 0.491900  [35904/60000]\n",
            "loss: 0.614244  [36544/60000]\n",
            "loss: 0.551993  [37184/60000]\n",
            "loss: 0.502850  [37824/60000]\n",
            "loss: 0.692772  [38464/60000]\n",
            "loss: 0.646246  [39104/60000]\n",
            "loss: 0.517969  [39744/60000]\n",
            "loss: 0.538107  [40384/60000]\n",
            "loss: 0.394209  [41024/60000]\n",
            "loss: 0.424953  [41664/60000]\n",
            "loss: 0.473327  [42304/60000]\n",
            "loss: 0.527164  [42944/60000]\n",
            "loss: 0.382351  [43584/60000]\n",
            "loss: 0.512732  [44224/60000]\n",
            "loss: 0.594592  [44864/60000]\n",
            "loss: 0.551327  [45504/60000]\n",
            "loss: 0.506837  [46144/60000]\n",
            "loss: 0.672601  [46784/60000]\n",
            "loss: 0.630570  [47424/60000]\n",
            "loss: 0.462531  [48064/60000]\n",
            "loss: 0.531215  [48704/60000]\n",
            "loss: 0.624966  [49344/60000]\n",
            "loss: 0.408444  [49984/60000]\n",
            "loss: 0.571272  [50624/60000]\n",
            "loss: 0.489606  [51264/60000]\n",
            "loss: 0.506640  [51904/60000]\n",
            "loss: 0.541023  [52544/60000]\n",
            "loss: 0.382426  [53184/60000]\n",
            "loss: 0.671494  [53824/60000]\n",
            "loss: 0.732627  [54464/60000]\n",
            "loss: 0.436954  [55104/60000]\n",
            "loss: 0.533031  [55744/60000]\n",
            "loss: 0.462703  [56384/60000]\n",
            "loss: 0.573034  [57024/60000]\n",
            "loss: 0.506909  [57664/60000]\n",
            "loss: 0.542114  [58304/60000]\n",
            "loss: 0.597379  [58944/60000]\n",
            "loss: 0.328955  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.537265 \n",
            "\n",
            "Epoch 8 \n",
            "---------------------------\n",
            "loss: 0.628387  [   64/60000]\n",
            "loss: 0.592607  [  704/60000]\n",
            "loss: 0.563184  [ 1344/60000]\n",
            "loss: 0.291733  [ 1984/60000]\n",
            "loss: 0.462147  [ 2624/60000]\n",
            "loss: 0.431367  [ 3264/60000]\n",
            "loss: 0.456208  [ 3904/60000]\n",
            "loss: 0.555580  [ 4544/60000]\n",
            "loss: 0.337789  [ 5184/60000]\n",
            "loss: 0.529049  [ 5824/60000]\n",
            "loss: 0.406222  [ 6464/60000]\n",
            "loss: 0.435295  [ 7104/60000]\n",
            "loss: 0.539533  [ 7744/60000]\n",
            "loss: 0.576786  [ 8384/60000]\n",
            "loss: 0.637787  [ 9024/60000]\n",
            "loss: 0.386709  [ 9664/60000]\n",
            "loss: 0.744362  [10304/60000]\n",
            "loss: 0.438129  [10944/60000]\n",
            "loss: 0.452438  [11584/60000]\n",
            "loss: 0.553223  [12224/60000]\n",
            "loss: 0.422135  [12864/60000]\n",
            "loss: 0.318965  [13504/60000]\n",
            "loss: 0.574390  [14144/60000]\n",
            "loss: 0.538140  [14784/60000]\n",
            "loss: 0.479697  [15424/60000]\n",
            "loss: 0.513625  [16064/60000]\n",
            "loss: 0.359073  [16704/60000]\n",
            "loss: 0.409699  [17344/60000]\n",
            "loss: 0.532193  [17984/60000]\n",
            "loss: 0.533593  [18624/60000]\n",
            "loss: 0.550787  [19264/60000]\n",
            "loss: 0.336912  [19904/60000]\n",
            "loss: 0.486312  [20544/60000]\n",
            "loss: 0.708864  [21184/60000]\n",
            "loss: 0.413022  [21824/60000]\n",
            "loss: 0.378474  [22464/60000]\n",
            "loss: 0.447408  [23104/60000]\n",
            "loss: 0.567259  [23744/60000]\n",
            "loss: 0.575021  [24384/60000]\n",
            "loss: 0.399195  [25024/60000]\n",
            "loss: 0.549965  [25664/60000]\n",
            "loss: 0.374946  [26304/60000]\n",
            "loss: 0.523629  [26944/60000]\n",
            "loss: 0.499045  [27584/60000]\n",
            "loss: 0.540775  [28224/60000]\n",
            "loss: 0.589869  [28864/60000]\n",
            "loss: 0.507513  [29504/60000]\n",
            "loss: 0.634549  [30144/60000]\n",
            "loss: 0.388535  [30784/60000]\n",
            "loss: 0.497421  [31424/60000]\n",
            "loss: 0.681981  [32064/60000]\n",
            "loss: 0.426272  [32704/60000]\n",
            "loss: 0.577089  [33344/60000]\n",
            "loss: 0.555240  [33984/60000]\n",
            "loss: 0.545430  [34624/60000]\n",
            "loss: 0.474237  [35264/60000]\n",
            "loss: 0.692908  [35904/60000]\n",
            "loss: 0.543657  [36544/60000]\n",
            "loss: 0.597160  [37184/60000]\n",
            "loss: 0.525418  [37824/60000]\n",
            "loss: 0.490198  [38464/60000]\n",
            "loss: 0.510909  [39104/60000]\n",
            "loss: 0.363350  [39744/60000]\n",
            "loss: 0.378766  [40384/60000]\n",
            "loss: 0.468863  [41024/60000]\n",
            "loss: 0.541857  [41664/60000]\n",
            "loss: 0.464233  [42304/60000]\n",
            "loss: 0.456374  [42944/60000]\n",
            "loss: 0.544672  [43584/60000]\n",
            "loss: 0.537764  [44224/60000]\n",
            "loss: 0.552706  [44864/60000]\n",
            "loss: 0.564862  [45504/60000]\n",
            "loss: 0.521275  [46144/60000]\n",
            "loss: 0.382246  [46784/60000]\n",
            "loss: 0.556463  [47424/60000]\n",
            "loss: 0.598899  [48064/60000]\n",
            "loss: 0.464424  [48704/60000]\n",
            "loss: 0.443497  [49344/60000]\n",
            "loss: 0.666602  [49984/60000]\n",
            "loss: 0.366121  [50624/60000]\n",
            "loss: 0.655028  [51264/60000]\n",
            "loss: 0.432761  [51904/60000]\n",
            "loss: 0.454186  [52544/60000]\n",
            "loss: 0.471377  [53184/60000]\n",
            "loss: 0.409926  [53824/60000]\n",
            "loss: 0.522817  [54464/60000]\n",
            "loss: 0.442282  [55104/60000]\n",
            "loss: 0.752010  [55744/60000]\n",
            "loss: 0.408840  [56384/60000]\n",
            "loss: 0.458200  [57024/60000]\n",
            "loss: 0.530507  [57664/60000]\n",
            "loss: 0.437952  [58304/60000]\n",
            "loss: 0.333455  [58944/60000]\n",
            "loss: 0.653765  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.528708 \n",
            "\n",
            "Epoch 9 \n",
            "---------------------------\n",
            "loss: 0.558350  [   64/60000]\n",
            "loss: 0.475746  [  704/60000]\n",
            "loss: 0.407731  [ 1344/60000]\n",
            "loss: 0.331978  [ 1984/60000]\n",
            "loss: 0.382431  [ 2624/60000]\n",
            "loss: 0.328299  [ 3264/60000]\n",
            "loss: 0.526767  [ 3904/60000]\n",
            "loss: 0.613099  [ 4544/60000]\n",
            "loss: 0.701740  [ 5184/60000]\n",
            "loss: 0.411002  [ 5824/60000]\n",
            "loss: 0.549107  [ 6464/60000]\n",
            "loss: 0.541677  [ 7104/60000]\n",
            "loss: 0.514178  [ 7744/60000]\n",
            "loss: 0.486333  [ 8384/60000]\n",
            "loss: 0.627957  [ 9024/60000]\n",
            "loss: 0.546024  [ 9664/60000]\n",
            "loss: 0.429585  [10304/60000]\n",
            "loss: 0.382695  [10944/60000]\n",
            "loss: 0.323778  [11584/60000]\n",
            "loss: 0.502428  [12224/60000]\n",
            "loss: 0.450316  [12864/60000]\n",
            "loss: 0.595604  [13504/60000]\n",
            "loss: 0.624812  [14144/60000]\n",
            "loss: 0.446301  [14784/60000]\n",
            "loss: 0.606307  [15424/60000]\n",
            "loss: 0.582596  [16064/60000]\n",
            "loss: 0.253737  [16704/60000]\n",
            "loss: 0.490691  [17344/60000]\n",
            "loss: 0.344862  [17984/60000]\n",
            "loss: 0.470031  [18624/60000]\n",
            "loss: 0.490685  [19264/60000]\n",
            "loss: 0.373046  [19904/60000]\n",
            "loss: 0.398294  [20544/60000]\n",
            "loss: 0.401370  [21184/60000]\n",
            "loss: 0.470136  [21824/60000]\n",
            "loss: 0.516286  [22464/60000]\n",
            "loss: 0.535436  [23104/60000]\n",
            "loss: 0.583080  [23744/60000]\n",
            "loss: 0.610573  [24384/60000]\n",
            "loss: 0.582582  [25024/60000]\n",
            "loss: 0.723172  [25664/60000]\n",
            "loss: 0.496791  [26304/60000]\n",
            "loss: 0.694678  [26944/60000]\n",
            "loss: 0.521822  [27584/60000]\n",
            "loss: 0.328511  [28224/60000]\n",
            "loss: 0.520601  [28864/60000]\n",
            "loss: 0.493434  [29504/60000]\n",
            "loss: 0.520088  [30144/60000]\n",
            "loss: 0.538063  [30784/60000]\n",
            "loss: 0.537164  [31424/60000]\n",
            "loss: 0.548929  [32064/60000]\n",
            "loss: 0.478272  [32704/60000]\n",
            "loss: 0.550827  [33344/60000]\n",
            "loss: 0.544116  [33984/60000]\n",
            "loss: 0.360007  [34624/60000]\n",
            "loss: 0.428485  [35264/60000]\n",
            "loss: 0.426372  [35904/60000]\n",
            "loss: 0.461082  [36544/60000]\n",
            "loss: 0.343883  [37184/60000]\n",
            "loss: 0.586693  [37824/60000]\n",
            "loss: 0.397591  [38464/60000]\n",
            "loss: 0.383436  [39104/60000]\n",
            "loss: 0.542972  [39744/60000]\n",
            "loss: 0.578939  [40384/60000]\n",
            "loss: 0.453375  [41024/60000]\n",
            "loss: 0.428717  [41664/60000]\n",
            "loss: 0.679726  [42304/60000]\n",
            "loss: 0.388312  [42944/60000]\n",
            "loss: 0.393493  [43584/60000]\n",
            "loss: 0.566626  [44224/60000]\n",
            "loss: 0.470575  [44864/60000]\n",
            "loss: 0.641016  [45504/60000]\n",
            "loss: 0.536130  [46144/60000]\n",
            "loss: 0.430558  [46784/60000]\n",
            "loss: 0.624474  [47424/60000]\n",
            "loss: 0.466834  [48064/60000]\n",
            "loss: 0.496570  [48704/60000]\n",
            "loss: 0.508419  [49344/60000]\n",
            "loss: 0.608174  [49984/60000]\n",
            "loss: 0.595785  [50624/60000]\n",
            "loss: 0.503352  [51264/60000]\n",
            "loss: 0.573221  [51904/60000]\n",
            "loss: 0.527996  [52544/60000]\n",
            "loss: 0.525854  [53184/60000]\n",
            "loss: 0.535838  [53824/60000]\n",
            "loss: 0.601492  [54464/60000]\n",
            "loss: 0.446965  [55104/60000]\n",
            "loss: 0.458067  [55744/60000]\n",
            "loss: 0.432034  [56384/60000]\n",
            "loss: 0.594923  [57024/60000]\n",
            "loss: 0.568414  [57664/60000]\n",
            "loss: 0.437440  [58304/60000]\n",
            "loss: 0.369805  [58944/60000]\n",
            "loss: 0.402332  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.520979 \n",
            "\n",
            "Epoch 10 \n",
            "---------------------------\n",
            "loss: 0.447918  [   64/60000]\n",
            "loss: 0.307972  [  704/60000]\n",
            "loss: 0.573969  [ 1344/60000]\n",
            "loss: 0.417126  [ 1984/60000]\n",
            "loss: 0.453912  [ 2624/60000]\n",
            "loss: 0.522640  [ 3264/60000]\n",
            "loss: 0.600739  [ 3904/60000]\n",
            "loss: 0.684256  [ 4544/60000]\n",
            "loss: 0.484833  [ 5184/60000]\n",
            "loss: 0.674736  [ 5824/60000]\n",
            "loss: 0.628505  [ 6464/60000]\n",
            "loss: 0.474732  [ 7104/60000]\n",
            "loss: 0.386523  [ 7744/60000]\n",
            "loss: 0.466288  [ 8384/60000]\n",
            "loss: 0.421004  [ 9024/60000]\n",
            "loss: 0.427960  [ 9664/60000]\n",
            "loss: 0.506535  [10304/60000]\n",
            "loss: 0.481851  [10944/60000]\n",
            "loss: 0.360729  [11584/60000]\n",
            "loss: 0.426136  [12224/60000]\n",
            "loss: 0.604124  [12864/60000]\n",
            "loss: 0.358037  [13504/60000]\n",
            "loss: 0.443880  [14144/60000]\n",
            "loss: 0.512507  [14784/60000]\n",
            "loss: 0.479377  [15424/60000]\n",
            "loss: 0.402601  [16064/60000]\n",
            "loss: 0.494205  [16704/60000]\n",
            "loss: 0.344676  [17344/60000]\n",
            "loss: 0.496813  [17984/60000]\n",
            "loss: 0.520422  [18624/60000]\n",
            "loss: 0.365435  [19264/60000]\n",
            "loss: 0.554831  [19904/60000]\n",
            "loss: 0.446477  [20544/60000]\n",
            "loss: 0.480371  [21184/60000]\n",
            "loss: 0.349209  [21824/60000]\n",
            "loss: 0.505596  [22464/60000]\n",
            "loss: 0.449060  [23104/60000]\n",
            "loss: 0.287044  [23744/60000]\n",
            "loss: 0.555062  [24384/60000]\n",
            "loss: 0.589244  [25024/60000]\n",
            "loss: 0.580690  [25664/60000]\n",
            "loss: 0.516857  [26304/60000]\n",
            "loss: 0.602055  [26944/60000]\n",
            "loss: 0.380829  [27584/60000]\n",
            "loss: 0.817589  [28224/60000]\n",
            "loss: 0.425627  [28864/60000]\n",
            "loss: 0.490484  [29504/60000]\n",
            "loss: 0.511352  [30144/60000]\n",
            "loss: 0.372744  [30784/60000]\n",
            "loss: 0.537927  [31424/60000]\n",
            "loss: 0.465858  [32064/60000]\n",
            "loss: 0.514430  [32704/60000]\n",
            "loss: 0.533109  [33344/60000]\n",
            "loss: 0.434844  [33984/60000]\n",
            "loss: 0.730990  [34624/60000]\n",
            "loss: 0.533987  [35264/60000]\n",
            "loss: 0.424430  [35904/60000]\n",
            "loss: 0.567900  [36544/60000]\n",
            "loss: 0.422070  [37184/60000]\n",
            "loss: 0.527849  [37824/60000]\n",
            "loss: 0.607253  [38464/60000]\n",
            "loss: 0.529796  [39104/60000]\n",
            "loss: 0.482600  [39744/60000]\n",
            "loss: 0.430436  [40384/60000]\n",
            "loss: 0.437346  [41024/60000]\n",
            "loss: 0.415612  [41664/60000]\n",
            "loss: 0.403644  [42304/60000]\n",
            "loss: 0.352294  [42944/60000]\n",
            "loss: 0.450269  [43584/60000]\n",
            "loss: 0.679280  [44224/60000]\n",
            "loss: 0.464215  [44864/60000]\n",
            "loss: 0.400829  [45504/60000]\n",
            "loss: 0.475288  [46144/60000]\n",
            "loss: 0.482632  [46784/60000]\n",
            "loss: 0.416439  [47424/60000]\n",
            "loss: 0.547983  [48064/60000]\n",
            "loss: 0.653848  [48704/60000]\n",
            "loss: 0.487510  [49344/60000]\n",
            "loss: 0.539782  [49984/60000]\n",
            "loss: 0.519568  [50624/60000]\n",
            "loss: 0.466325  [51264/60000]\n",
            "loss: 0.414215  [51904/60000]\n",
            "loss: 0.563915  [52544/60000]\n",
            "loss: 0.508678  [53184/60000]\n",
            "loss: 0.376249  [53824/60000]\n",
            "loss: 0.460733  [54464/60000]\n",
            "loss: 0.379800  [55104/60000]\n",
            "loss: 0.494278  [55744/60000]\n",
            "loss: 0.503512  [56384/60000]\n",
            "loss: 0.378000  [57024/60000]\n",
            "loss: 0.428295  [57664/60000]\n",
            "loss: 0.405462  [58304/60000]\n",
            "loss: 0.421499  [58944/60000]\n",
            "loss: 0.656043  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.515763 \n",
            "\n",
            "Epoch 11 \n",
            "---------------------------\n",
            "loss: 0.554803  [   64/60000]\n",
            "loss: 0.482346  [  704/60000]\n",
            "loss: 0.472802  [ 1344/60000]\n",
            "loss: 0.507468  [ 1984/60000]\n",
            "loss: 0.273965  [ 2624/60000]\n",
            "loss: 0.553581  [ 3264/60000]\n",
            "loss: 0.533284  [ 3904/60000]\n",
            "loss: 0.387222  [ 4544/60000]\n",
            "loss: 0.514237  [ 5184/60000]\n",
            "loss: 0.516135  [ 5824/60000]\n",
            "loss: 0.626157  [ 6464/60000]\n",
            "loss: 0.452963  [ 7104/60000]\n",
            "loss: 0.371032  [ 7744/60000]\n",
            "loss: 0.514540  [ 8384/60000]\n",
            "loss: 0.438160  [ 9024/60000]\n",
            "loss: 0.517578  [ 9664/60000]\n",
            "loss: 0.584104  [10304/60000]\n",
            "loss: 0.571101  [10944/60000]\n",
            "loss: 0.583434  [11584/60000]\n",
            "loss: 0.581113  [12224/60000]\n",
            "loss: 0.385742  [12864/60000]\n",
            "loss: 0.324911  [13504/60000]\n",
            "loss: 0.507431  [14144/60000]\n",
            "loss: 0.584420  [14784/60000]\n",
            "loss: 0.367449  [15424/60000]\n",
            "loss: 0.548081  [16064/60000]\n",
            "loss: 0.571543  [16704/60000]\n",
            "loss: 0.755532  [17344/60000]\n",
            "loss: 0.531286  [17984/60000]\n",
            "loss: 0.636955  [18624/60000]\n",
            "loss: 0.547249  [19264/60000]\n",
            "loss: 0.491989  [19904/60000]\n",
            "loss: 0.397460  [20544/60000]\n",
            "loss: 0.348039  [21184/60000]\n",
            "loss: 0.571847  [21824/60000]\n",
            "loss: 0.626597  [22464/60000]\n",
            "loss: 0.565742  [23104/60000]\n",
            "loss: 0.719772  [23744/60000]\n",
            "loss: 0.455417  [24384/60000]\n",
            "loss: 0.535131  [25024/60000]\n",
            "loss: 0.327385  [25664/60000]\n",
            "loss: 0.605600  [26304/60000]\n",
            "loss: 0.772247  [26944/60000]\n",
            "loss: 0.493022  [27584/60000]\n",
            "loss: 0.345210  [28224/60000]\n",
            "loss: 0.518837  [28864/60000]\n",
            "loss: 0.578779  [29504/60000]\n",
            "loss: 0.368292  [30144/60000]\n",
            "loss: 0.453516  [30784/60000]\n",
            "loss: 0.467856  [31424/60000]\n",
            "loss: 0.461568  [32064/60000]\n",
            "loss: 0.509079  [32704/60000]\n",
            "loss: 0.670786  [33344/60000]\n",
            "loss: 0.423025  [33984/60000]\n",
            "loss: 0.425168  [34624/60000]\n",
            "loss: 0.488307  [35264/60000]\n",
            "loss: 0.523150  [35904/60000]\n",
            "loss: 0.444361  [36544/60000]\n",
            "loss: 0.535568  [37184/60000]\n",
            "loss: 0.367527  [37824/60000]\n",
            "loss: 0.588041  [38464/60000]\n",
            "loss: 0.360350  [39104/60000]\n",
            "loss: 0.482003  [39744/60000]\n",
            "loss: 0.698497  [40384/60000]\n",
            "loss: 0.407440  [41024/60000]\n",
            "loss: 0.393029  [41664/60000]\n",
            "loss: 0.505954  [42304/60000]\n",
            "loss: 0.507994  [42944/60000]\n",
            "loss: 0.388260  [43584/60000]\n",
            "loss: 0.543957  [44224/60000]\n",
            "loss: 0.406089  [44864/60000]\n",
            "loss: 0.324603  [45504/60000]\n",
            "loss: 0.567951  [46144/60000]\n",
            "loss: 0.761111  [46784/60000]\n",
            "loss: 0.418254  [47424/60000]\n",
            "loss: 0.543426  [48064/60000]\n",
            "loss: 0.566672  [48704/60000]\n",
            "loss: 0.541846  [49344/60000]\n",
            "loss: 0.479036  [49984/60000]\n",
            "loss: 0.595430  [50624/60000]\n",
            "loss: 0.382870  [51264/60000]\n",
            "loss: 0.573288  [51904/60000]\n",
            "loss: 0.427850  [52544/60000]\n",
            "loss: 0.572549  [53184/60000]\n",
            "loss: 0.408924  [53824/60000]\n",
            "loss: 0.464707  [54464/60000]\n",
            "loss: 0.485652  [55104/60000]\n",
            "loss: 0.415506  [55744/60000]\n",
            "loss: 0.289008  [56384/60000]\n",
            "loss: 0.412029  [57024/60000]\n",
            "loss: 0.549512  [57664/60000]\n",
            "loss: 0.503721  [58304/60000]\n",
            "loss: 0.332587  [58944/60000]\n",
            "loss: 0.386599  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.514111 \n",
            "\n",
            "Epoch 12 \n",
            "---------------------------\n",
            "loss: 0.506789  [   64/60000]\n",
            "loss: 0.416701  [  704/60000]\n",
            "loss: 0.442943  [ 1344/60000]\n",
            "loss: 0.496615  [ 1984/60000]\n",
            "loss: 0.535473  [ 2624/60000]\n",
            "loss: 0.548448  [ 3264/60000]\n",
            "loss: 0.650312  [ 3904/60000]\n",
            "loss: 0.599034  [ 4544/60000]\n",
            "loss: 0.467706  [ 5184/60000]\n",
            "loss: 0.523622  [ 5824/60000]\n",
            "loss: 0.638030  [ 6464/60000]\n",
            "loss: 0.387345  [ 7104/60000]\n",
            "loss: 0.430167  [ 7744/60000]\n",
            "loss: 0.679265  [ 8384/60000]\n",
            "loss: 0.370242  [ 9024/60000]\n",
            "loss: 0.526484  [ 9664/60000]\n",
            "loss: 0.467464  [10304/60000]\n",
            "loss: 0.504426  [10944/60000]\n",
            "loss: 0.469175  [11584/60000]\n",
            "loss: 0.366329  [12224/60000]\n",
            "loss: 0.375619  [12864/60000]\n",
            "loss: 0.491540  [13504/60000]\n",
            "loss: 0.419671  [14144/60000]\n",
            "loss: 0.616486  [14784/60000]\n",
            "loss: 0.470794  [15424/60000]\n",
            "loss: 0.668688  [16064/60000]\n",
            "loss: 0.528851  [16704/60000]\n",
            "loss: 0.528963  [17344/60000]\n",
            "loss: 0.749918  [17984/60000]\n",
            "loss: 0.638048  [18624/60000]\n",
            "loss: 0.317129  [19264/60000]\n",
            "loss: 0.483515  [19904/60000]\n",
            "loss: 0.494849  [20544/60000]\n",
            "loss: 0.368044  [21184/60000]\n",
            "loss: 0.438016  [21824/60000]\n",
            "loss: 0.431276  [22464/60000]\n",
            "loss: 0.544843  [23104/60000]\n",
            "loss: 0.469517  [23744/60000]\n",
            "loss: 0.323777  [24384/60000]\n",
            "loss: 0.596683  [25024/60000]\n",
            "loss: 0.356729  [25664/60000]\n",
            "loss: 0.620154  [26304/60000]\n",
            "loss: 0.503181  [26944/60000]\n",
            "loss: 0.511005  [27584/60000]\n",
            "loss: 0.478624  [28224/60000]\n",
            "loss: 0.306154  [28864/60000]\n",
            "loss: 0.392392  [29504/60000]\n",
            "loss: 0.414136  [30144/60000]\n",
            "loss: 0.430862  [30784/60000]\n",
            "loss: 0.328315  [31424/60000]\n",
            "loss: 0.365686  [32064/60000]\n",
            "loss: 0.425778  [32704/60000]\n",
            "loss: 0.645033  [33344/60000]\n",
            "loss: 0.619244  [33984/60000]\n",
            "loss: 0.418045  [34624/60000]\n",
            "loss: 0.632970  [35264/60000]\n",
            "loss: 0.419715  [35904/60000]\n",
            "loss: 0.591773  [36544/60000]\n",
            "loss: 0.446287  [37184/60000]\n",
            "loss: 0.452494  [37824/60000]\n",
            "loss: 0.598183  [38464/60000]\n",
            "loss: 0.474117  [39104/60000]\n",
            "loss: 0.388271  [39744/60000]\n",
            "loss: 0.450126  [40384/60000]\n",
            "loss: 0.505760  [41024/60000]\n",
            "loss: 0.611031  [41664/60000]\n",
            "loss: 0.466015  [42304/60000]\n",
            "loss: 0.564330  [42944/60000]\n",
            "loss: 0.319117  [43584/60000]\n",
            "loss: 0.283216  [44224/60000]\n",
            "loss: 0.551383  [44864/60000]\n",
            "loss: 0.444097  [45504/60000]\n",
            "loss: 0.340347  [46144/60000]\n",
            "loss: 0.337859  [46784/60000]\n",
            "loss: 0.586740  [47424/60000]\n",
            "loss: 0.395141  [48064/60000]\n",
            "loss: 0.382220  [48704/60000]\n",
            "loss: 0.449232  [49344/60000]\n",
            "loss: 0.319695  [49984/60000]\n",
            "loss: 0.304709  [50624/60000]\n",
            "loss: 0.389558  [51264/60000]\n",
            "loss: 0.342134  [51904/60000]\n",
            "loss: 0.439457  [52544/60000]\n",
            "loss: 0.654553  [53184/60000]\n",
            "loss: 0.390179  [53824/60000]\n",
            "loss: 0.426429  [54464/60000]\n",
            "loss: 0.366357  [55104/60000]\n",
            "loss: 0.479350  [55744/60000]\n",
            "loss: 0.556449  [56384/60000]\n",
            "loss: 0.482615  [57024/60000]\n",
            "loss: 0.442877  [57664/60000]\n",
            "loss: 0.396342  [58304/60000]\n",
            "loss: 0.397102  [58944/60000]\n",
            "loss: 0.435170  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.505405 \n",
            "\n",
            "Epoch 13 \n",
            "---------------------------\n",
            "loss: 0.550126  [   64/60000]\n",
            "loss: 0.401745  [  704/60000]\n",
            "loss: 0.597233  [ 1344/60000]\n",
            "loss: 0.505125  [ 1984/60000]\n",
            "loss: 0.632971  [ 2624/60000]\n",
            "loss: 0.470141  [ 3264/60000]\n",
            "loss: 0.707527  [ 3904/60000]\n",
            "loss: 0.347011  [ 4544/60000]\n",
            "loss: 0.641461  [ 5184/60000]\n",
            "loss: 0.430744  [ 5824/60000]\n",
            "loss: 0.571347  [ 6464/60000]\n",
            "loss: 0.448411  [ 7104/60000]\n",
            "loss: 0.608412  [ 7744/60000]\n",
            "loss: 0.445329  [ 8384/60000]\n",
            "loss: 0.374232  [ 9024/60000]\n",
            "loss: 0.546558  [ 9664/60000]\n",
            "loss: 0.541953  [10304/60000]\n",
            "loss: 0.556684  [10944/60000]\n",
            "loss: 0.400203  [11584/60000]\n",
            "loss: 0.602828  [12224/60000]\n",
            "loss: 0.313104  [12864/60000]\n",
            "loss: 0.459725  [13504/60000]\n",
            "loss: 0.490591  [14144/60000]\n",
            "loss: 0.404973  [14784/60000]\n",
            "loss: 0.427354  [15424/60000]\n",
            "loss: 0.483230  [16064/60000]\n",
            "loss: 0.432963  [16704/60000]\n",
            "loss: 0.645762  [17344/60000]\n",
            "loss: 0.439675  [17984/60000]\n",
            "loss: 0.484758  [18624/60000]\n",
            "loss: 0.353039  [19264/60000]\n",
            "loss: 0.593986  [19904/60000]\n",
            "loss: 0.494635  [20544/60000]\n",
            "loss: 0.460799  [21184/60000]\n",
            "loss: 0.403758  [21824/60000]\n",
            "loss: 0.559381  [22464/60000]\n",
            "loss: 0.366610  [23104/60000]\n",
            "loss: 0.583226  [23744/60000]\n",
            "loss: 0.363470  [24384/60000]\n",
            "loss: 0.365187  [25024/60000]\n",
            "loss: 0.636451  [25664/60000]\n",
            "loss: 0.358911  [26304/60000]\n",
            "loss: 0.386648  [26944/60000]\n",
            "loss: 0.366522  [27584/60000]\n",
            "loss: 0.366865  [28224/60000]\n",
            "loss: 0.410816  [28864/60000]\n",
            "loss: 0.396197  [29504/60000]\n",
            "loss: 0.267851  [30144/60000]\n",
            "loss: 0.653755  [30784/60000]\n",
            "loss: 0.457788  [31424/60000]\n",
            "loss: 0.398349  [32064/60000]\n",
            "loss: 0.524238  [32704/60000]\n",
            "loss: 0.312507  [33344/60000]\n",
            "loss: 0.303966  [33984/60000]\n",
            "loss: 0.450160  [34624/60000]\n",
            "loss: 0.447830  [35264/60000]\n",
            "loss: 0.464389  [35904/60000]\n",
            "loss: 0.505164  [36544/60000]\n",
            "loss: 0.435670  [37184/60000]\n",
            "loss: 0.566594  [37824/60000]\n",
            "loss: 0.360362  [38464/60000]\n",
            "loss: 0.476363  [39104/60000]\n",
            "loss: 0.559457  [39744/60000]\n",
            "loss: 0.458824  [40384/60000]\n",
            "loss: 0.572737  [41024/60000]\n",
            "loss: 0.391923  [41664/60000]\n",
            "loss: 0.442952  [42304/60000]\n",
            "loss: 0.444849  [42944/60000]\n",
            "loss: 0.533216  [43584/60000]\n",
            "loss: 0.414939  [44224/60000]\n",
            "loss: 0.509044  [44864/60000]\n",
            "loss: 0.466991  [45504/60000]\n",
            "loss: 0.402655  [46144/60000]\n",
            "loss: 0.671449  [46784/60000]\n",
            "loss: 0.590996  [47424/60000]\n",
            "loss: 0.513143  [48064/60000]\n",
            "loss: 0.481127  [48704/60000]\n",
            "loss: 0.599276  [49344/60000]\n",
            "loss: 0.577551  [49984/60000]\n",
            "loss: 0.652314  [50624/60000]\n",
            "loss: 0.404343  [51264/60000]\n",
            "loss: 0.457293  [51904/60000]\n",
            "loss: 0.427184  [52544/60000]\n",
            "loss: 0.391400  [53184/60000]\n",
            "loss: 0.632042  [53824/60000]\n",
            "loss: 0.457865  [54464/60000]\n",
            "loss: 0.434526  [55104/60000]\n",
            "loss: 0.628078  [55744/60000]\n",
            "loss: 0.434286  [56384/60000]\n",
            "loss: 0.537415  [57024/60000]\n",
            "loss: 0.504841  [57664/60000]\n",
            "loss: 0.496090  [58304/60000]\n",
            "loss: 0.475289  [58944/60000]\n",
            "loss: 0.493235  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.504653 \n",
            "\n",
            "Epoch 14 \n",
            "---------------------------\n",
            "loss: 0.551919  [   64/60000]\n",
            "loss: 0.553040  [  704/60000]\n",
            "loss: 0.335056  [ 1344/60000]\n",
            "loss: 0.417657  [ 1984/60000]\n",
            "loss: 0.533737  [ 2624/60000]\n",
            "loss: 0.514353  [ 3264/60000]\n",
            "loss: 0.276065  [ 3904/60000]\n",
            "loss: 0.504901  [ 4544/60000]\n",
            "loss: 0.700941  [ 5184/60000]\n",
            "loss: 0.460666  [ 5824/60000]\n",
            "loss: 0.376970  [ 6464/60000]\n",
            "loss: 0.479835  [ 7104/60000]\n",
            "loss: 0.397002  [ 7744/60000]\n",
            "loss: 0.497284  [ 8384/60000]\n",
            "loss: 0.404891  [ 9024/60000]\n",
            "loss: 0.539362  [ 9664/60000]\n",
            "loss: 0.416498  [10304/60000]\n",
            "loss: 0.744291  [10944/60000]\n",
            "loss: 0.404196  [11584/60000]\n",
            "loss: 0.363200  [12224/60000]\n",
            "loss: 0.501972  [12864/60000]\n",
            "loss: 0.463074  [13504/60000]\n",
            "loss: 0.515923  [14144/60000]\n",
            "loss: 0.519821  [14784/60000]\n",
            "loss: 0.385753  [15424/60000]\n",
            "loss: 0.358549  [16064/60000]\n",
            "loss: 0.430007  [16704/60000]\n",
            "loss: 0.743992  [17344/60000]\n",
            "loss: 0.364435  [17984/60000]\n",
            "loss: 0.551237  [18624/60000]\n",
            "loss: 0.345193  [19264/60000]\n",
            "loss: 0.434821  [19904/60000]\n",
            "loss: 0.443267  [20544/60000]\n",
            "loss: 0.318931  [21184/60000]\n",
            "loss: 0.671667  [21824/60000]\n",
            "loss: 0.459734  [22464/60000]\n",
            "loss: 0.420608  [23104/60000]\n",
            "loss: 0.450033  [23744/60000]\n",
            "loss: 0.656923  [24384/60000]\n",
            "loss: 0.511932  [25024/60000]\n",
            "loss: 0.390215  [25664/60000]\n",
            "loss: 0.594783  [26304/60000]\n",
            "loss: 0.720666  [26944/60000]\n",
            "loss: 0.398858  [27584/60000]\n",
            "loss: 0.427681  [28224/60000]\n",
            "loss: 0.527833  [28864/60000]\n",
            "loss: 0.399968  [29504/60000]\n",
            "loss: 0.484472  [30144/60000]\n",
            "loss: 0.473545  [30784/60000]\n",
            "loss: 0.740471  [31424/60000]\n",
            "loss: 0.592757  [32064/60000]\n",
            "loss: 0.333101  [32704/60000]\n",
            "loss: 0.358510  [33344/60000]\n",
            "loss: 0.335779  [33984/60000]\n",
            "loss: 0.532583  [34624/60000]\n",
            "loss: 0.590894  [35264/60000]\n",
            "loss: 0.634205  [35904/60000]\n",
            "loss: 0.324034  [36544/60000]\n",
            "loss: 0.464417  [37184/60000]\n",
            "loss: 0.556758  [37824/60000]\n",
            "loss: 0.280694  [38464/60000]\n",
            "loss: 0.541813  [39104/60000]\n",
            "loss: 0.514754  [39744/60000]\n",
            "loss: 0.365665  [40384/60000]\n",
            "loss: 0.423960  [41024/60000]\n",
            "loss: 0.303723  [41664/60000]\n",
            "loss: 0.524045  [42304/60000]\n",
            "loss: 0.560670  [42944/60000]\n",
            "loss: 0.653363  [43584/60000]\n",
            "loss: 0.406750  [44224/60000]\n",
            "loss: 0.607460  [44864/60000]\n",
            "loss: 0.370369  [45504/60000]\n",
            "loss: 0.449561  [46144/60000]\n",
            "loss: 0.612259  [46784/60000]\n",
            "loss: 0.539293  [47424/60000]\n",
            "loss: 0.725711  [48064/60000]\n",
            "loss: 0.351636  [48704/60000]\n",
            "loss: 0.462656  [49344/60000]\n",
            "loss: 0.578955  [49984/60000]\n",
            "loss: 0.539570  [50624/60000]\n",
            "loss: 0.456152  [51264/60000]\n",
            "loss: 0.473229  [51904/60000]\n",
            "loss: 0.470717  [52544/60000]\n",
            "loss: 0.330436  [53184/60000]\n",
            "loss: 0.535431  [53824/60000]\n",
            "loss: 0.493415  [54464/60000]\n",
            "loss: 0.428512  [55104/60000]\n",
            "loss: 0.744271  [55744/60000]\n",
            "loss: 0.532948  [56384/60000]\n",
            "loss: 0.300471  [57024/60000]\n",
            "loss: 0.776825  [57664/60000]\n",
            "loss: 0.428586  [58304/60000]\n",
            "loss: 0.407692  [58944/60000]\n",
            "loss: 0.398344  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.498744 \n",
            "\n",
            "Epoch 15 \n",
            "---------------------------\n",
            "loss: 0.493746  [   64/60000]\n",
            "loss: 0.383884  [  704/60000]\n",
            "loss: 0.494866  [ 1344/60000]\n",
            "loss: 0.506016  [ 1984/60000]\n",
            "loss: 0.296014  [ 2624/60000]\n",
            "loss: 0.496624  [ 3264/60000]\n",
            "loss: 0.404820  [ 3904/60000]\n",
            "loss: 0.573762  [ 4544/60000]\n",
            "loss: 0.494343  [ 5184/60000]\n",
            "loss: 0.477712  [ 5824/60000]\n",
            "loss: 0.430026  [ 6464/60000]\n",
            "loss: 0.334191  [ 7104/60000]\n",
            "loss: 0.630452  [ 7744/60000]\n",
            "loss: 0.405738  [ 8384/60000]\n",
            "loss: 0.480955  [ 9024/60000]\n",
            "loss: 0.545957  [ 9664/60000]\n",
            "loss: 0.692644  [10304/60000]\n",
            "loss: 0.619204  [10944/60000]\n",
            "loss: 0.471389  [11584/60000]\n",
            "loss: 0.401110  [12224/60000]\n",
            "loss: 0.470957  [12864/60000]\n",
            "loss: 0.355753  [13504/60000]\n",
            "loss: 0.394965  [14144/60000]\n",
            "loss: 0.666456  [14784/60000]\n",
            "loss: 0.633786  [15424/60000]\n",
            "loss: 0.449142  [16064/60000]\n",
            "loss: 0.358294  [16704/60000]\n",
            "loss: 0.750821  [17344/60000]\n",
            "loss: 0.673101  [17984/60000]\n",
            "loss: 0.435060  [18624/60000]\n",
            "loss: 0.465773  [19264/60000]\n",
            "loss: 0.512122  [19904/60000]\n",
            "loss: 0.510059  [20544/60000]\n",
            "loss: 0.400494  [21184/60000]\n",
            "loss: 0.477560  [21824/60000]\n",
            "loss: 0.443645  [22464/60000]\n",
            "loss: 0.282692  [23104/60000]\n",
            "loss: 0.542364  [23744/60000]\n",
            "loss: 0.464147  [24384/60000]\n",
            "loss: 0.351418  [25024/60000]\n",
            "loss: 0.619185  [25664/60000]\n",
            "loss: 0.378075  [26304/60000]\n",
            "loss: 0.310012  [26944/60000]\n",
            "loss: 0.558061  [27584/60000]\n",
            "loss: 0.362183  [28224/60000]\n",
            "loss: 0.604368  [28864/60000]\n",
            "loss: 0.586899  [29504/60000]\n",
            "loss: 0.365483  [30144/60000]\n",
            "loss: 0.396560  [30784/60000]\n",
            "loss: 0.396816  [31424/60000]\n",
            "loss: 0.454263  [32064/60000]\n",
            "loss: 0.419413  [32704/60000]\n",
            "loss: 0.617407  [33344/60000]\n",
            "loss: 0.429469  [33984/60000]\n",
            "loss: 0.413242  [34624/60000]\n",
            "loss: 0.382729  [35264/60000]\n",
            "loss: 0.369603  [35904/60000]\n",
            "loss: 0.572160  [36544/60000]\n",
            "loss: 0.388313  [37184/60000]\n",
            "loss: 0.461283  [37824/60000]\n",
            "loss: 0.593164  [38464/60000]\n",
            "loss: 0.423827  [39104/60000]\n",
            "loss: 0.454186  [39744/60000]\n",
            "loss: 0.582131  [40384/60000]\n",
            "loss: 0.706711  [41024/60000]\n",
            "loss: 0.616197  [41664/60000]\n",
            "loss: 0.453475  [42304/60000]\n",
            "loss: 0.410548  [42944/60000]\n",
            "loss: 0.595235  [43584/60000]\n",
            "loss: 0.390156  [44224/60000]\n",
            "loss: 0.743456  [44864/60000]\n",
            "loss: 0.448921  [45504/60000]\n",
            "loss: 0.592064  [46144/60000]\n",
            "loss: 0.504152  [46784/60000]\n",
            "loss: 0.506780  [47424/60000]\n",
            "loss: 0.467326  [48064/60000]\n",
            "loss: 0.509435  [48704/60000]\n",
            "loss: 0.351736  [49344/60000]\n",
            "loss: 0.718085  [49984/60000]\n",
            "loss: 0.425083  [50624/60000]\n",
            "loss: 0.523824  [51264/60000]\n",
            "loss: 0.541175  [51904/60000]\n",
            "loss: 0.428536  [52544/60000]\n",
            "loss: 0.380616  [53184/60000]\n",
            "loss: 0.360397  [53824/60000]\n",
            "loss: 0.605781  [54464/60000]\n",
            "loss: 0.500344  [55104/60000]\n",
            "loss: 0.303607  [55744/60000]\n",
            "loss: 0.379569  [56384/60000]\n",
            "loss: 0.516548  [57024/60000]\n",
            "loss: 0.432538  [57664/60000]\n",
            "loss: 0.466193  [58304/60000]\n",
            "loss: 0.444536  [58944/60000]\n",
            "loss: 0.454125  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.496844 \n",
            "\n",
            "Epoch 16 \n",
            "---------------------------\n",
            "loss: 0.506636  [   64/60000]\n",
            "loss: 0.501113  [  704/60000]\n",
            "loss: 0.385818  [ 1344/60000]\n",
            "loss: 0.462920  [ 1984/60000]\n",
            "loss: 0.674615  [ 2624/60000]\n",
            "loss: 0.431293  [ 3264/60000]\n",
            "loss: 0.275087  [ 3904/60000]\n",
            "loss: 0.393368  [ 4544/60000]\n",
            "loss: 0.607592  [ 5184/60000]\n",
            "loss: 0.409873  [ 5824/60000]\n",
            "loss: 0.582475  [ 6464/60000]\n",
            "loss: 0.498874  [ 7104/60000]\n",
            "loss: 0.506859  [ 7744/60000]\n",
            "loss: 0.449275  [ 8384/60000]\n",
            "loss: 0.487634  [ 9024/60000]\n",
            "loss: 0.354942  [ 9664/60000]\n",
            "loss: 0.484380  [10304/60000]\n",
            "loss: 0.577919  [10944/60000]\n",
            "loss: 0.403610  [11584/60000]\n",
            "loss: 0.425323  [12224/60000]\n",
            "loss: 0.448390  [12864/60000]\n",
            "loss: 0.525530  [13504/60000]\n",
            "loss: 0.367218  [14144/60000]\n",
            "loss: 0.514934  [14784/60000]\n",
            "loss: 0.386769  [15424/60000]\n",
            "loss: 0.491961  [16064/60000]\n",
            "loss: 0.397765  [16704/60000]\n",
            "loss: 0.380586  [17344/60000]\n",
            "loss: 0.305783  [17984/60000]\n",
            "loss: 0.430955  [18624/60000]\n",
            "loss: 0.431232  [19264/60000]\n",
            "loss: 0.525750  [19904/60000]\n",
            "loss: 0.359410  [20544/60000]\n",
            "loss: 0.571050  [21184/60000]\n",
            "loss: 0.547367  [21824/60000]\n",
            "loss: 0.667053  [22464/60000]\n",
            "loss: 0.411889  [23104/60000]\n",
            "loss: 0.513205  [23744/60000]\n",
            "loss: 0.391232  [24384/60000]\n",
            "loss: 0.453655  [25024/60000]\n",
            "loss: 0.422879  [25664/60000]\n",
            "loss: 0.451235  [26304/60000]\n",
            "loss: 0.441802  [26944/60000]\n",
            "loss: 0.482021  [27584/60000]\n",
            "loss: 0.405578  [28224/60000]\n",
            "loss: 0.544902  [28864/60000]\n",
            "loss: 0.563667  [29504/60000]\n",
            "loss: 0.372709  [30144/60000]\n",
            "loss: 0.326860  [30784/60000]\n",
            "loss: 0.472019  [31424/60000]\n",
            "loss: 0.537903  [32064/60000]\n",
            "loss: 0.769866  [32704/60000]\n",
            "loss: 0.460914  [33344/60000]\n",
            "loss: 0.415322  [33984/60000]\n",
            "loss: 0.380634  [34624/60000]\n",
            "loss: 0.429572  [35264/60000]\n",
            "loss: 0.495132  [35904/60000]\n",
            "loss: 0.476523  [36544/60000]\n",
            "loss: 0.350320  [37184/60000]\n",
            "loss: 0.472599  [37824/60000]\n",
            "loss: 0.359966  [38464/60000]\n",
            "loss: 0.307432  [39104/60000]\n",
            "loss: 0.355078  [39744/60000]\n",
            "loss: 0.409715  [40384/60000]\n",
            "loss: 0.588797  [41024/60000]\n",
            "loss: 0.492525  [41664/60000]\n",
            "loss: 0.465796  [42304/60000]\n",
            "loss: 0.440597  [42944/60000]\n",
            "loss: 0.286149  [43584/60000]\n",
            "loss: 0.456630  [44224/60000]\n",
            "loss: 0.545612  [44864/60000]\n",
            "loss: 0.484601  [45504/60000]\n",
            "loss: 0.404583  [46144/60000]\n",
            "loss: 0.461146  [46784/60000]\n",
            "loss: 0.464158  [47424/60000]\n",
            "loss: 0.314457  [48064/60000]\n",
            "loss: 0.705812  [48704/60000]\n",
            "loss: 0.447166  [49344/60000]\n",
            "loss: 0.570880  [49984/60000]\n",
            "loss: 0.396600  [50624/60000]\n",
            "loss: 0.443644  [51264/60000]\n",
            "loss: 0.624617  [51904/60000]\n",
            "loss: 0.470608  [52544/60000]\n",
            "loss: 0.589099  [53184/60000]\n",
            "loss: 0.483597  [53824/60000]\n",
            "loss: 0.354594  [54464/60000]\n",
            "loss: 0.346115  [55104/60000]\n",
            "loss: 0.520876  [55744/60000]\n",
            "loss: 0.556454  [56384/60000]\n",
            "loss: 0.442604  [57024/60000]\n",
            "loss: 0.387729  [57664/60000]\n",
            "loss: 0.575220  [58304/60000]\n",
            "loss: 0.422728  [58944/60000]\n",
            "loss: 0.288851  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.493177 \n",
            "\n",
            "Epoch 17 \n",
            "---------------------------\n",
            "loss: 0.520682  [   64/60000]\n",
            "loss: 0.296035  [  704/60000]\n",
            "loss: 0.413372  [ 1344/60000]\n",
            "loss: 0.410799  [ 1984/60000]\n",
            "loss: 0.826302  [ 2624/60000]\n",
            "loss: 0.439895  [ 3264/60000]\n",
            "loss: 0.343891  [ 3904/60000]\n",
            "loss: 0.514874  [ 4544/60000]\n",
            "loss: 0.470017  [ 5184/60000]\n",
            "loss: 0.449251  [ 5824/60000]\n",
            "loss: 0.399726  [ 6464/60000]\n",
            "loss: 0.496285  [ 7104/60000]\n",
            "loss: 0.398448  [ 7744/60000]\n",
            "loss: 0.421973  [ 8384/60000]\n",
            "loss: 0.546497  [ 9024/60000]\n",
            "loss: 0.281544  [ 9664/60000]\n",
            "loss: 0.417611  [10304/60000]\n",
            "loss: 0.380951  [10944/60000]\n",
            "loss: 0.352471  [11584/60000]\n",
            "loss: 0.497805  [12224/60000]\n",
            "loss: 0.634878  [12864/60000]\n",
            "loss: 0.479593  [13504/60000]\n",
            "loss: 0.459623  [14144/60000]\n",
            "loss: 0.469596  [14784/60000]\n",
            "loss: 0.492629  [15424/60000]\n",
            "loss: 0.367355  [16064/60000]\n",
            "loss: 0.401462  [16704/60000]\n",
            "loss: 0.358347  [17344/60000]\n",
            "loss: 0.453342  [17984/60000]\n",
            "loss: 0.491654  [18624/60000]\n",
            "loss: 0.551861  [19264/60000]\n",
            "loss: 0.315261  [19904/60000]\n",
            "loss: 0.438823  [20544/60000]\n",
            "loss: 0.501087  [21184/60000]\n",
            "loss: 0.388226  [21824/60000]\n",
            "loss: 0.508364  [22464/60000]\n",
            "loss: 0.468829  [23104/60000]\n",
            "loss: 0.476618  [23744/60000]\n",
            "loss: 0.637216  [24384/60000]\n",
            "loss: 0.564576  [25024/60000]\n",
            "loss: 0.763982  [25664/60000]\n",
            "loss: 0.611304  [26304/60000]\n",
            "loss: 0.557709  [26944/60000]\n",
            "loss: 0.381892  [27584/60000]\n",
            "loss: 0.547734  [28224/60000]\n",
            "loss: 0.412073  [28864/60000]\n",
            "loss: 0.450306  [29504/60000]\n",
            "loss: 0.296121  [30144/60000]\n",
            "loss: 0.514473  [30784/60000]\n",
            "loss: 0.438429  [31424/60000]\n",
            "loss: 0.294521  [32064/60000]\n",
            "loss: 0.473548  [32704/60000]\n",
            "loss: 0.418179  [33344/60000]\n",
            "loss: 0.375219  [33984/60000]\n",
            "loss: 0.400195  [34624/60000]\n",
            "loss: 0.408167  [35264/60000]\n",
            "loss: 0.417507  [35904/60000]\n",
            "loss: 0.652679  [36544/60000]\n",
            "loss: 0.348394  [37184/60000]\n",
            "loss: 0.347987  [37824/60000]\n",
            "loss: 0.392140  [38464/60000]\n",
            "loss: 0.263750  [39104/60000]\n",
            "loss: 0.509517  [39744/60000]\n",
            "loss: 0.564897  [40384/60000]\n",
            "loss: 0.353634  [41024/60000]\n",
            "loss: 0.387253  [41664/60000]\n",
            "loss: 0.398857  [42304/60000]\n",
            "loss: 0.520263  [42944/60000]\n",
            "loss: 0.468784  [43584/60000]\n",
            "loss: 0.387355  [44224/60000]\n",
            "loss: 0.501412  [44864/60000]\n",
            "loss: 0.366264  [45504/60000]\n",
            "loss: 0.344462  [46144/60000]\n",
            "loss: 0.558403  [46784/60000]\n",
            "loss: 0.567314  [47424/60000]\n",
            "loss: 0.676591  [48064/60000]\n",
            "loss: 0.353287  [48704/60000]\n",
            "loss: 0.432950  [49344/60000]\n",
            "loss: 0.264017  [49984/60000]\n",
            "loss: 0.451960  [50624/60000]\n",
            "loss: 0.631716  [51264/60000]\n",
            "loss: 0.447258  [51904/60000]\n",
            "loss: 0.432414  [52544/60000]\n",
            "loss: 0.408779  [53184/60000]\n",
            "loss: 0.325812  [53824/60000]\n",
            "loss: 0.581660  [54464/60000]\n",
            "loss: 0.378097  [55104/60000]\n",
            "loss: 0.418008  [55744/60000]\n",
            "loss: 0.467267  [56384/60000]\n",
            "loss: 0.443999  [57024/60000]\n",
            "loss: 0.455854  [57664/60000]\n",
            "loss: 0.430211  [58304/60000]\n",
            "loss: 0.446334  [58944/60000]\n",
            "loss: 0.642633  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.489903 \n",
            "\n",
            "Epoch 18 \n",
            "---------------------------\n",
            "loss: 0.488160  [   64/60000]\n",
            "loss: 0.524642  [  704/60000]\n",
            "loss: 0.623604  [ 1344/60000]\n",
            "loss: 0.427505  [ 1984/60000]\n",
            "loss: 0.499167  [ 2624/60000]\n",
            "loss: 0.305446  [ 3264/60000]\n",
            "loss: 0.528938  [ 3904/60000]\n",
            "loss: 0.403142  [ 4544/60000]\n",
            "loss: 0.563684  [ 5184/60000]\n",
            "loss: 0.282281  [ 5824/60000]\n",
            "loss: 0.388070  [ 6464/60000]\n",
            "loss: 0.554143  [ 7104/60000]\n",
            "loss: 0.542240  [ 7744/60000]\n",
            "loss: 0.444499  [ 8384/60000]\n",
            "loss: 0.518027  [ 9024/60000]\n",
            "loss: 0.356352  [ 9664/60000]\n",
            "loss: 0.556227  [10304/60000]\n",
            "loss: 0.421718  [10944/60000]\n",
            "loss: 0.437440  [11584/60000]\n",
            "loss: 0.335302  [12224/60000]\n",
            "loss: 0.310080  [12864/60000]\n",
            "loss: 0.484516  [13504/60000]\n",
            "loss: 0.439881  [14144/60000]\n",
            "loss: 0.308552  [14784/60000]\n",
            "loss: 0.617666  [15424/60000]\n",
            "loss: 0.520682  [16064/60000]\n",
            "loss: 0.481702  [16704/60000]\n",
            "loss: 0.448037  [17344/60000]\n",
            "loss: 0.306016  [17984/60000]\n",
            "loss: 0.379200  [18624/60000]\n",
            "loss: 0.492837  [19264/60000]\n",
            "loss: 0.485549  [19904/60000]\n",
            "loss: 0.401092  [20544/60000]\n",
            "loss: 0.598131  [21184/60000]\n",
            "loss: 0.418336  [21824/60000]\n",
            "loss: 0.382256  [22464/60000]\n",
            "loss: 0.392481  [23104/60000]\n",
            "loss: 0.380693  [23744/60000]\n",
            "loss: 0.493790  [24384/60000]\n",
            "loss: 0.489537  [25024/60000]\n",
            "loss: 0.421883  [25664/60000]\n",
            "loss: 0.673273  [26304/60000]\n",
            "loss: 0.401274  [26944/60000]\n",
            "loss: 0.481733  [27584/60000]\n",
            "loss: 0.647723  [28224/60000]\n",
            "loss: 0.397466  [28864/60000]\n",
            "loss: 0.541211  [29504/60000]\n",
            "loss: 0.444721  [30144/60000]\n",
            "loss: 0.572447  [30784/60000]\n",
            "loss: 0.414895  [31424/60000]\n",
            "loss: 0.474324  [32064/60000]\n",
            "loss: 0.494548  [32704/60000]\n",
            "loss: 0.314591  [33344/60000]\n",
            "loss: 0.557109  [33984/60000]\n",
            "loss: 0.476514  [34624/60000]\n",
            "loss: 0.422624  [35264/60000]\n",
            "loss: 0.601161  [35904/60000]\n",
            "loss: 0.312314  [36544/60000]\n",
            "loss: 0.405914  [37184/60000]\n",
            "loss: 0.496713  [37824/60000]\n",
            "loss: 0.509457  [38464/60000]\n",
            "loss: 0.478813  [39104/60000]\n",
            "loss: 0.452638  [39744/60000]\n",
            "loss: 0.368976  [40384/60000]\n",
            "loss: 0.447514  [41024/60000]\n",
            "loss: 0.460915  [41664/60000]\n",
            "loss: 0.657918  [42304/60000]\n",
            "loss: 0.439085  [42944/60000]\n",
            "loss: 0.480112  [43584/60000]\n",
            "loss: 0.439684  [44224/60000]\n",
            "loss: 0.427706  [44864/60000]\n",
            "loss: 0.583262  [45504/60000]\n",
            "loss: 0.408785  [46144/60000]\n",
            "loss: 0.411255  [46784/60000]\n",
            "loss: 0.470131  [47424/60000]\n",
            "loss: 0.334978  [48064/60000]\n",
            "loss: 0.531617  [48704/60000]\n",
            "loss: 0.335349  [49344/60000]\n",
            "loss: 0.440744  [49984/60000]\n",
            "loss: 0.671122  [50624/60000]\n",
            "loss: 0.367838  [51264/60000]\n",
            "loss: 0.663710  [51904/60000]\n",
            "loss: 0.485708  [52544/60000]\n",
            "loss: 0.564430  [53184/60000]\n",
            "loss: 0.450941  [53824/60000]\n",
            "loss: 0.471542  [54464/60000]\n",
            "loss: 0.319015  [55104/60000]\n",
            "loss: 0.421803  [55744/60000]\n",
            "loss: 0.429016  [56384/60000]\n",
            "loss: 0.483828  [57024/60000]\n",
            "loss: 0.660041  [57664/60000]\n",
            "loss: 0.417856  [58304/60000]\n",
            "loss: 0.567940  [58944/60000]\n",
            "loss: 0.479706  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.487416 \n",
            "\n",
            "Epoch 19 \n",
            "---------------------------\n",
            "loss: 0.391497  [   64/60000]\n",
            "loss: 0.412582  [  704/60000]\n",
            "loss: 0.658456  [ 1344/60000]\n",
            "loss: 0.393818  [ 1984/60000]\n",
            "loss: 0.344351  [ 2624/60000]\n",
            "loss: 0.357850  [ 3264/60000]\n",
            "loss: 0.466860  [ 3904/60000]\n",
            "loss: 0.373931  [ 4544/60000]\n",
            "loss: 0.553575  [ 5184/60000]\n",
            "loss: 0.534836  [ 5824/60000]\n",
            "loss: 0.468184  [ 6464/60000]\n",
            "loss: 0.425687  [ 7104/60000]\n",
            "loss: 0.365401  [ 7744/60000]\n",
            "loss: 0.503745  [ 8384/60000]\n",
            "loss: 0.359428  [ 9024/60000]\n",
            "loss: 0.467434  [ 9664/60000]\n",
            "loss: 0.366361  [10304/60000]\n",
            "loss: 0.392250  [10944/60000]\n",
            "loss: 0.482190  [11584/60000]\n",
            "loss: 0.399552  [12224/60000]\n",
            "loss: 0.671121  [12864/60000]\n",
            "loss: 0.418869  [13504/60000]\n",
            "loss: 0.641454  [14144/60000]\n",
            "loss: 0.433919  [14784/60000]\n",
            "loss: 0.361264  [15424/60000]\n",
            "loss: 0.439352  [16064/60000]\n",
            "loss: 0.308902  [16704/60000]\n",
            "loss: 0.436817  [17344/60000]\n",
            "loss: 0.377823  [17984/60000]\n",
            "loss: 0.504195  [18624/60000]\n",
            "loss: 0.584129  [19264/60000]\n",
            "loss: 0.451467  [19904/60000]\n",
            "loss: 0.523542  [20544/60000]\n",
            "loss: 0.419162  [21184/60000]\n",
            "loss: 0.554572  [21824/60000]\n",
            "loss: 0.224427  [22464/60000]\n",
            "loss: 0.440560  [23104/60000]\n",
            "loss: 0.569244  [23744/60000]\n",
            "loss: 0.644519  [24384/60000]\n",
            "loss: 0.543341  [25024/60000]\n",
            "loss: 0.566877  [25664/60000]\n",
            "loss: 0.440601  [26304/60000]\n",
            "loss: 0.561592  [26944/60000]\n",
            "loss: 0.385491  [27584/60000]\n",
            "loss: 0.415932  [28224/60000]\n",
            "loss: 0.512590  [28864/60000]\n",
            "loss: 0.464287  [29504/60000]\n",
            "loss: 0.439400  [30144/60000]\n",
            "loss: 0.444152  [30784/60000]\n",
            "loss: 0.507133  [31424/60000]\n",
            "loss: 0.290612  [32064/60000]\n",
            "loss: 0.492743  [32704/60000]\n",
            "loss: 0.389146  [33344/60000]\n",
            "loss: 0.548004  [33984/60000]\n",
            "loss: 0.468097  [34624/60000]\n",
            "loss: 0.440658  [35264/60000]\n",
            "loss: 0.338855  [35904/60000]\n",
            "loss: 0.450282  [36544/60000]\n",
            "loss: 0.393585  [37184/60000]\n",
            "loss: 0.551326  [37824/60000]\n",
            "loss: 0.513925  [38464/60000]\n",
            "loss: 0.476247  [39104/60000]\n",
            "loss: 0.579416  [39744/60000]\n",
            "loss: 0.524758  [40384/60000]\n",
            "loss: 0.649641  [41024/60000]\n",
            "loss: 0.552281  [41664/60000]\n",
            "loss: 0.514316  [42304/60000]\n",
            "loss: 0.484093  [42944/60000]\n",
            "loss: 0.481803  [43584/60000]\n",
            "loss: 0.440366  [44224/60000]\n",
            "loss: 0.340550  [44864/60000]\n",
            "loss: 0.426723  [45504/60000]\n",
            "loss: 0.559995  [46144/60000]\n",
            "loss: 0.630075  [46784/60000]\n",
            "loss: 0.329127  [47424/60000]\n",
            "loss: 0.356796  [48064/60000]\n",
            "loss: 0.463707  [48704/60000]\n",
            "loss: 0.341144  [49344/60000]\n",
            "loss: 0.379089  [49984/60000]\n",
            "loss: 0.269422  [50624/60000]\n",
            "loss: 0.429659  [51264/60000]\n",
            "loss: 0.351192  [51904/60000]\n",
            "loss: 0.481501  [52544/60000]\n",
            "loss: 0.419908  [53184/60000]\n",
            "loss: 0.429134  [53824/60000]\n",
            "loss: 0.378271  [54464/60000]\n",
            "loss: 0.186226  [55104/60000]\n",
            "loss: 0.373270  [55744/60000]\n",
            "loss: 0.416859  [56384/60000]\n",
            "loss: 0.546352  [57024/60000]\n",
            "loss: 0.542080  [57664/60000]\n",
            "loss: 0.583958  [58304/60000]\n",
            "loss: 0.467691  [58944/60000]\n",
            "loss: 0.505110  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.484301 \n",
            "\n",
            "Epoch 20 \n",
            "---------------------------\n",
            "loss: 0.463957  [   64/60000]\n",
            "loss: 0.496642  [  704/60000]\n",
            "loss: 0.473769  [ 1344/60000]\n",
            "loss: 0.435591  [ 1984/60000]\n",
            "loss: 0.442448  [ 2624/60000]\n",
            "loss: 0.253820  [ 3264/60000]\n",
            "loss: 0.413772  [ 3904/60000]\n",
            "loss: 0.334822  [ 4544/60000]\n",
            "loss: 0.521975  [ 5184/60000]\n",
            "loss: 0.555184  [ 5824/60000]\n",
            "loss: 0.556650  [ 6464/60000]\n",
            "loss: 0.729365  [ 7104/60000]\n",
            "loss: 0.498590  [ 7744/60000]\n",
            "loss: 0.400295  [ 8384/60000]\n",
            "loss: 0.688717  [ 9024/60000]\n",
            "loss: 0.464036  [ 9664/60000]\n",
            "loss: 0.416386  [10304/60000]\n",
            "loss: 0.403376  [10944/60000]\n",
            "loss: 0.418771  [11584/60000]\n",
            "loss: 0.602102  [12224/60000]\n",
            "loss: 0.432703  [12864/60000]\n",
            "loss: 0.586920  [13504/60000]\n",
            "loss: 0.368796  [14144/60000]\n",
            "loss: 0.618037  [14784/60000]\n",
            "loss: 0.467541  [15424/60000]\n",
            "loss: 0.417953  [16064/60000]\n",
            "loss: 0.353137  [16704/60000]\n",
            "loss: 0.580410  [17344/60000]\n",
            "loss: 0.641145  [17984/60000]\n",
            "loss: 0.275350  [18624/60000]\n",
            "loss: 0.481008  [19264/60000]\n",
            "loss: 0.383504  [19904/60000]\n",
            "loss: 0.393500  [20544/60000]\n",
            "loss: 0.345263  [21184/60000]\n",
            "loss: 0.370427  [21824/60000]\n",
            "loss: 0.343225  [22464/60000]\n",
            "loss: 0.512281  [23104/60000]\n",
            "loss: 0.628029  [23744/60000]\n",
            "loss: 0.494350  [24384/60000]\n",
            "loss: 0.372820  [25024/60000]\n",
            "loss: 0.479316  [25664/60000]\n",
            "loss: 0.438093  [26304/60000]\n",
            "loss: 0.330338  [26944/60000]\n",
            "loss: 0.370959  [27584/60000]\n",
            "loss: 0.521468  [28224/60000]\n",
            "loss: 0.338152  [28864/60000]\n",
            "loss: 0.478450  [29504/60000]\n",
            "loss: 0.368978  [30144/60000]\n",
            "loss: 0.347695  [30784/60000]\n",
            "loss: 0.474345  [31424/60000]\n",
            "loss: 0.352167  [32064/60000]\n",
            "loss: 0.584237  [32704/60000]\n",
            "loss: 0.369393  [33344/60000]\n",
            "loss: 0.338198  [33984/60000]\n",
            "loss: 0.478166  [34624/60000]\n",
            "loss: 0.536494  [35264/60000]\n",
            "loss: 0.408450  [35904/60000]\n",
            "loss: 0.274627  [36544/60000]\n",
            "loss: 0.490188  [37184/60000]\n",
            "loss: 0.308800  [37824/60000]\n",
            "loss: 0.577255  [38464/60000]\n",
            "loss: 0.574067  [39104/60000]\n",
            "loss: 0.566249  [39744/60000]\n",
            "loss: 0.392013  [40384/60000]\n",
            "loss: 0.333129  [41024/60000]\n",
            "loss: 0.520408  [41664/60000]\n",
            "loss: 0.539469  [42304/60000]\n",
            "loss: 0.316632  [42944/60000]\n",
            "loss: 0.404782  [43584/60000]\n",
            "loss: 0.423419  [44224/60000]\n",
            "loss: 0.383280  [44864/60000]\n",
            "loss: 0.379958  [45504/60000]\n",
            "loss: 0.400233  [46144/60000]\n",
            "loss: 0.406035  [46784/60000]\n",
            "loss: 0.383740  [47424/60000]\n",
            "loss: 0.363860  [48064/60000]\n",
            "loss: 0.419580  [48704/60000]\n",
            "loss: 0.407171  [49344/60000]\n",
            "loss: 0.571680  [49984/60000]\n",
            "loss: 0.566195  [50624/60000]\n",
            "loss: 0.352600  [51264/60000]\n",
            "loss: 0.408695  [51904/60000]\n",
            "loss: 0.388440  [52544/60000]\n",
            "loss: 0.431953  [53184/60000]\n",
            "loss: 0.532109  [53824/60000]\n",
            "loss: 0.492213  [54464/60000]\n",
            "loss: 0.447758  [55104/60000]\n",
            "loss: 0.509921  [55744/60000]\n",
            "loss: 0.577896  [56384/60000]\n",
            "loss: 0.408284  [57024/60000]\n",
            "loss: 0.274557  [57664/60000]\n",
            "loss: 0.573378  [58304/60000]\n",
            "loss: 0.409861  [58944/60000]\n",
            "loss: 0.631975  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.484147 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 10. Saving the Model\n",
        "# -----------------------------\n",
        "EPOCH = epochs\n",
        "PATH = \"model1.pt\"\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss_fn,\n",
        "            }, PATH)"
      ],
      "metadata": {
        "id": "k1cv4z0pI5eY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 11. Loading Data Back In\n",
        "# -----------------------------\n",
        "PATH = \"model1.pt\"\n",
        "model = FirstNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "checkpoint = torch.load(PATH, weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "EPOCH = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odM0KDkZJQpt",
        "outputId": "78d7dfa4-5698-4dd4-f81a-19f52976c863"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FirstNet(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear_relu_model): Sequential(\n",
              "    (0): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}